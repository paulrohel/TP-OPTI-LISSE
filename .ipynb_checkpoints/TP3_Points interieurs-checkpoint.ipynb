{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Méthode de point intérieur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de ce TP est d'utiliser des méthodes de point intérieur pour minimiser une fonction sous contrainte.\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\umin}[1]{\\underset{#1}{\\min}\\;}$\n",
    "$\\DeclareMathOperator{\\eqdef}{\\overset{\\tiny def}{=}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit $f$ une fonction régulière de $\\mathbb{R}^n$ dans $\\mathbb{R}$, nous nous intéresserons à deux types de problème : \n",
    "1. Le problème à contrainte linéaire \n",
    "$$\n",
    "    (\\mathcal{P}_{\\infty,\\text{ lin.}}) \\qquad \\umin{x\\in \\mathbb{R}^n, A x \\leq b} f(x)\n",
    "$$\n",
    "où $A$ est une matrice $\\mathcal M_{pn}(\\mathbb{R})$ et $b\\in \\mathbb{R}^p$.\n",
    "2. Le problème avec **une** contrainte non linéaire\n",
    "$$\n",
    "    (\\mathcal{P}_{\\infty,\\text{ non lin.}}) \\qquad \\umin{x\\in \\mathbb{R}^n, g(x)\\le 0} f(x)\n",
    "$$\n",
    "Où $g$ est une fonction régulière de $\\mathbb{R}^n$ dans $\\mathbb{R}$.\n",
    "\n",
    "\n",
    "L'idée des méthodes de point intérieur est d'approximer les problèmes $(\\mathcal P_{\\infty,\\bullet})$ en utilisant des [fonctions barrières logarithmiques](https://en.wikipedia.org/wiki/Barrier_function)\n",
    "\n",
    "$$\n",
    "\t(\\mathcal{P}_{t, \\text{non-lin.}}) \\qquad \\umin{x\\in \\mathbb{R}^d, } f_t(x) \\eqdef f(x) - \\frac{1}{t} \\text{ln}( -g(x)  )\n",
    "$$\n",
    "ou \n",
    "$$\n",
    "\t(\\mathcal{P}_{t, \\text{ lin.}}) \\qquad \\umin{x\\in \\mathbb{R}^d, } f_t(x) \\eqdef f(x) - \\frac{1}{t} \\text{Log}( b-A x  )\n",
    "$$\n",
    "avec\n",
    "$$\n",
    "\t\\text{Log}(u) \\eqdef \\sum_i \\ln(u_i)\n",
    "$$\n",
    "\n",
    "Ainsi la fonction -Log ou -log qui est une fonction strictement concave, agit comme une barrière pour la contrainte. On s'attend à ce qu'à la limite, quand $t$ tend vers $+\\infty$, le problème $(\\mathcal P_{t,\\bullet})$ tende vers le problème $(\\mathcal P_{\\infty,\\bullet})$.\n",
    "## I-Contrainte non-linéaire\n",
    "On suppose dans cette section que l'on se place dans le cadre d'une seule contrainte non-linéaire.\n",
    "Si $f$ et $g$ sont données et $f_t=f - \\frac{1}{t} \\text{log}( -g)$, \n",
    "  \n",
    "<div class=\"alert alert-block alert-info\"> Calculez ci-dessous le gradient et la Hessienne de $f_t$ en fonction des gradients et de la Hessienne de $f$ et de $g$ \n",
    "$$\\nabla f_t(x) = ?? \\quad H [f_t](x) = ?? $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On suppose que `f` et `g` sont des fonctions définies avec une classe dans le fichier `functions.py` (ainsi les calculs du gradient ou de la Hessienne de `f` et de `g` sont déjà faits). Créer ci-dessous une classe `non_lin_cst` qui calcule `f_t`, son gradient ou sa Hessienne. La classe prend à la construction la fonction `f`, la fonction `g` et la valeur de `t`. Le constructeur de la classe est déjà implémenté.\n",
    "Attention, dans le cas où `g(x)` est $>0$, la fonction `value(self,x)` doit rendre `np.inf` et ne doit pas rendre une erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import functions_corr as func\n",
    "import Optim as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class non_lin_cst() :\n",
    "    def __init__(self,f,g,t) :\n",
    "        self.zeros()\n",
    "        self.f=f\n",
    "        self.g=g\n",
    "        self.t=t\n",
    "        self.nb_constraints=1. # nombre de contraintes du problème\n",
    "    def zeros(self) :\n",
    "        self.nb_eval=0 # number of evaluations of the function self.value()\n",
    "        self.nb_grad=0 # number of evaluations of the function self.grad()\n",
    "        self.nb_hess=0 # number of evaluations of the function self.Hess()\n",
    "    def value(self,x) :\n",
    "        # returns the value of the function at point x\n",
    "        return None\n",
    "    def grad(self,x) :\n",
    "        # returns the gradient of the function at point x\n",
    "        return None\n",
    "    def Hess(self,x) :\n",
    "        # returns the Hessian of the function at point x\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de tester notre classe, on utilise comme `f` la fonction Rosenbrock et comme `g` la fonction `square2` définie ci-dessous :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class square2() :\n",
    "    def __init__(self) :\n",
    "        self.zeros()\n",
    "    def zeros(self) :\n",
    "        self.nb_eval=0 # number of evaluations of the function self.value()\n",
    "        self.nb_grad=0 # number of evaluations of the function self.grad()\n",
    "        self.nb_hess=0 # number of evaluations of the function self.Hess()\n",
    "    def value(self,x) :\n",
    "        # returns the value of the function at point x\n",
    "        self.nb_eval+=1\n",
    "        return 0.5*x[0]**2+7/2.*x[1]**2-1\n",
    "    def grad(self,x) :\n",
    "        # returns the gradient of the function at point x\n",
    "        self.nb_grad+=1\n",
    "        return np.array([x[0],7*x[1]])\n",
    "    def Hess(self,x) :\n",
    "        # returns the Hessian of the function at point x\n",
    "        self.nb_hess+=1\n",
    "        to_return=np.zeros((2,2))\n",
    "        to_return[0,0]=1\n",
    "        to_return[1,1]=7\n",
    "        return to_return\n",
    "f_t=non_lin_cst(func.Rosen(),square2(),0.33)\n",
    "x_0=np.array([0.,0.])\n",
    "print('Doit être 1.0=',f_t.value(x_0))\n",
    "x_0=np.array([0.2,0.12])\n",
    "print('Doit être 1.5012148269226977=',f_t.value(x_0))\n",
    "x_0=np.array([2,1.3])\n",
    "print('Doit être inf =',f_t.value(x_0))\n",
    "print('######## TEST DE DERIVEE NUMERIQUE')\n",
    "a=np.array([0.2,0.12])\n",
    "d=np.random.randn(2)\n",
    "opt.deriv_num(f_t,a,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme d'optimisation\n",
    "Vous devez récupérer votre algorithme de Newton ainsi que la recherche linéaire de Wolfe (avec un step initial de $1$ ) dans la cellule ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_wolfe(x,function,step,descent,f,df) :\n",
    "    pass\n",
    "def ls_wolfe_step_is_one(x,function,step,descent,f,df) :\n",
    "    pass\n",
    "def dc_Newton(x,function,df) :\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant lancer une optimisation en utilisant la méthode de Newton avec un pas de Wolfe initialisé à $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_t=non_lin_cst(func.Rosen(),square2(),0.33)\n",
    "res=opt.main_algorithm(f_t,5,np.array([0,0]),dc_Newton,ls_wolfe_step_is_one,tol=1.e-7,verbose=True)\n",
    "final_x=res['list_x'][-1]\n",
    "print('x final=',final_x)\n",
    "# VOUS DEVEZ TROUVER\n",
    "#Fonction de Rosenbrock\n",
    "#Fonction (x,y) --> x^2/2+7/2*y^2-1\n",
    "#Fonction Interior point method\n",
    "#iter=   0 f=1.000e+00 df=2.000e+00 comp=[   1,   1,   0]\n",
    "#iter=   1 f=8.586e-01 df=8.192e+00 comp=[   3,   2,   1]\n",
    "#iter=   2 f=7.028e-01 df=3.443e-01 comp=[   4,   3,   2]\n",
    "#iter=   3 f=6.973e-01 df=2.196e-01 comp=[   5,   4,   3]\n",
    "#iter=   4 f=6.972e-01 df=1.166e-03 comp=[   6,   5,   4]\n",
    "#iter=   5 f=6.972e-01 df=6.426e-07 comp=[   7,   6,   5]\n",
    "#iter=   6 f=6.972e-01 df=3.988e-14 comp=[   8,   7,   6]\n",
    "#Success !!! Algorithm converged !!!\n",
    "#x final= [0.25636244 0.05915168]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La méthode du chemin central\n",
    "Notre objectif est d'envoyer le paramètre $t$ vers $+\\infty$. Pour ce faire on va suivre la méthode du chemin central, nous commençons par nous donner $t_1$ et nous notons $x_0$ le point initial et $x_1$ le minimiseur donné de $f_{t_1}$ par une méthode de Newton commençant à $x_0$. Ensuite nous multiplions $t_1$ par $\\mu$ pour obtenir $t_2$ et nous lançons une méthode de Newton commençant à $x_1$. On note $x_2$ le résultat obtenu. \n",
    "Ainsi nous allons construire une suite de points solutions du problème \n",
    "$$x_k \\in \\textrm{argmin} f_{t_k}(x).$$\n",
    "avec $t_{k}=\\mu t_{k-1}$ et $x_{k}$ une solution donnée par un algorithme de Newton (avec recherche de Wolfe) et avec comme point de départ $x_{k-1}$. L'idée d'utiliser comme initialisation la solution d'un problème d'optimisation s'appelle \"warm restart\".\n",
    "\n",
    "On peut montrer que l'erreur que l'on fait entre $x_k$ et $x^\\star$ (où $x^\\star$ est le minimum de $f$) vérifie :\n",
    "$$ 0 \\le f(x_k)-f(x^\\star)\\le \\frac{p}{t_k},$$\n",
    "où $p$ est le nombre de contraintes. Ainsi on va s'arrêter dès que $\\varepsilon t_k > p$ où $\\varepsilon$ est la précision de l'algorithme de Newton.\n",
    "Ecrire une fonction `def central_path(function,mu,varepsilon,x0)` qui implémente cette stratégie. Elle prend en argument \n",
    " 1. `function`: une instance de la classe `non_lin_cst`\n",
    " 2. `mu`: un réel $>1$ qui est le facteur multiplicatif de `t`\n",
    " 3. `varepsilon`: une précision\n",
    " 4. `x0`: un point de départ de la méthode\n",
    " \n",
    "Cette fonction doit rendre `costs,x` où\n",
    "\n",
    "1. `costs` est la liste des coûts pour toutes les itérations\n",
    "2. `x` est le point final d'arrivée\n",
    "\n",
    "On rappelle que la valeur de $p$ se trouve dans `function.nb_constraints` tandis que la valeur de $t$ se trouve dans `function.t`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_path(function,mu,varepsilon,x0) :\n",
    "    return costs,x\n",
    "\n",
    "f_t=non_lin_cst(func.Rosen(),square2(),10)\n",
    "costs,x=central_path(f_t,10,1.e-9,np.array([0,0]))\n",
    "cost_min=np.min(costs)\n",
    "print('minimal value ',cost_min) # minimal value  0.0994618583079908\n",
    "print('x',x) # x [0.68525439 0.46758135]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez tracer l'évolution des coûts en échelle log et exhiber la convergence linéaire de ces algorithmes\n",
    "<div class=\"alert alert-block alert-info\"> METTEZ ICI VOTRE ARGUMENT POUR JUSTIFIER DE LA CONVERGENCE LINEAIRE DE CET ALGORITHME\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant s'intéresser à l'influence de $\\mu$. On fixe maintenant `varepsilon` à $10^{-6}$ et on va tracer la courbe d'évolution de la fonction à minimiser (en échelle log, et en traçant la fonction auquelle on a retiré `costmin`) pour différentes valeurs de $\\mu$ dans `mu_list` ci-dessous. La valeur de `cost_min` est inchangée et est la valeur trouvée pour `varepsilon` à $10^{-9}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_list=[2,4,10,100,1000,5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyez-vous une grande différence quand $\\mu$ varie ?\n",
    "<div class=\"alert alert-block alert-info\"> METTEZ ICI VOTRE ARGUMENT \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problème avec multiples contraintes linéaires\n",
    "On s'intéresse maintenant aux problèmes de la forme\n",
    "$$\n",
    "\t(\\mathcal{P}_{t, \\text{ lin.}}) \\qquad \\umin{x\\in \\mathbb{R}^d, } f_t(x) \\eqdef f(x) - \\frac{1}{t} \\text{Log}( b-A x  ),\n",
    "$$\n",
    "qui approximent, quand $t$ tend vers $+\\infty$ des problèmes du genre\n",
    "$$\n",
    "    (\\mathcal{P}_{\\infty,\\text{ lin.}}) \\qquad \\umin{x\\in \\mathbb{R}^n, A x \\leq b} f(x)\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> Calculez ci-dessous le gradient et la Hessienne de $f_t$ en fonction de $A$,$b$,t et du gradient et de la Hessienne de $f$  \n",
    "$$\\nabla f_t(x) = ?? \\quad H[f_t](x) = ?? $$\n",
    "</div>\n",
    "\n",
    "Créez une classe de fonction `class lin_cst()` qui prend au constructeur la fonction `f`, la matrice `A`, le vecteur `b` le scalaire `t` et  et qui calcule la valeur de $f_t$. Cette fonction devra agir comme `class non_lin_cst()` et notamment avoir un attribut `nb_constraints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lin_cst() :\n",
    "    def __init__(self,f,A,b,t) :\n",
    "        pass\n",
    "    def zeros(self) :\n",
    "        pass\n",
    "    def value(self,x) :\n",
    "        pass\n",
    "    def grad(self,x) :\n",
    "        pass\n",
    "    def Hess(self,x) :\n",
    "        pass\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va tester notre fonction dans la case ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x=np.zeros(2)\n",
    "n=10\n",
    "A=np.random.randn(42,2)\n",
    "b=np.abs(np.random.randn(42))\n",
    "x2=np.random.randn(2)\n",
    "f_t=lin_cst(func.Rosen(),A,b,10)\n",
    "print(f_t.value(x),f_t.value(x2)) ## 4.717787999246487 inf\n",
    "print(f_t.grad(x),f_t.grad(x2)) ## [-9.36529883 -1.43185282] [-624.84901174 -313.83869195]\n",
    "print(f_t.Hess(x)) ## [[1624.61765301  896.38712898] [ 896.38712898  871.39523753]]\n",
    "print(f_t.Hess(x2)) ## [[2531.87148488  934.54404056][ 934.54404056  476.99362034]]\n",
    "\n",
    "print('## TEST DE DERIVEE NUMERIQUE##')\n",
    "d=np.random.randn(2)\n",
    "opt.deriv_num(f_t,5.e-3*x2,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'intéresse maintenant au problème particulier du [Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics)), pour retrouver un signal parcimonieux. Le problème du Lasso s'écrit $$\n",
    "        \\umin{w \\in \\mathbb{R}^p} \\frac{1}{2}\\Vert Bw-y \\Vert_2^2 + \\lambda \\Vert x \\Vert_1\n",
    "$$\n",
    "On suppose ici qu'on veut retrouver un signal $w_0$ que l'on sait être parcimonieux (beaucoup de coefficients nuls), quand on n'observe que $y=Bw_0+N$ où $B$ est l'opérateur d'observation dans $ \\mathbb{R}^{n \\times p}$ avec $n<<p$ et $N$ est un bruit.  On va supposer ici que $B$ est une matrice aléatoire Gaussienne, ce qui pose notre problème dans le cadre du [compressed sensing](https://en.wikipedia.org/wiki/Compressed_sensing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40\n",
    "p = 60\n",
    "np.random.seed(42)\n",
    "B = np.random.randn(n,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée `w0` le vecteur que l'on souhaite retrouver, qui est parcimonieux et on génère le signal $y=Bw_0+N$ où $N$ est un bruit gaussien additif. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "w0 = np.zeros(p) \n",
    "I=(p*np.random.rand(4)).astype(int)\n",
    "print(I)\n",
    "w0[I] = np.array([.8, -.6, .7, -.9])\n",
    "N = np.random.randn(n)*np.max(np.abs(B@w0))*.02\n",
    "y = (B@w0) + N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fixe $\\lambda = \\frac{\\lambda_{\\max}}{10}$ où $\\lambda_{\\max} = \\Vert B^\\top y \\Vert_\\infty$ est la valeur limite du paramètre pour laquelle on peut montrer que la solution du problème du Lasso est nulle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = np.max(np.abs(B.T @ y))/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On donne aussi un exemple d'utilisation de la fonction `stem` de matplotlib, utile pour représenter les vecteurs parcimonieux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stem( w0, linefmt='--k', markerfmt='ko', label='$x_0$' )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de re-écrire le problème du Lasso comme un problème d'optimisation lisse sans contrainte, on introduit, pour tout vecteur $w\\in \\mathbb{R}^p$, on  introduit le vecteur $x=(x_-,x_+)$ tel que $x_-=\\max(-w,0)$ et $x_+=\\max(w,0)$. On a ainsi toujours \n",
    "$$w = x_+ - x_-\\quad \\text{et}\\quad |w|=x_++x_-\\quad \\text{et}\\quad x\\ge 0.$$\n",
    "Le problème revient donc à minimiser, sous la contrainte $x\\ge 0$ et en décomposant $x=(x_+,x_-)$\n",
    "$$ \n",
    "    f(x) = \\frac{1}{2}\\Vert B(x_+-x_-)-y\\Vert_2^2 + \\lambda \\langle x,1\\rangle,\n",
    "$$ \n",
    "où $1$ est le vecteur rempli de $1$. Ainsi la contrainte s'écrit bien $Ax \\le b$ avec\n",
    "$A=-\\text{Id}_{2p}$, $b=0$.\n",
    "<div class=\"alert alert-block alert-info\"> Calculez ci-dessous le gradient et la Hessienne de $f$ en fonction de $B,y,\\lambda$ et de $x=(x_-,x_+)$  \n",
    "$$\\nabla f(x) = ?? \\quad H[f](x) = ?? $$\n",
    "</div>\n",
    "\n",
    "\n",
    "Remplissez une classe `class lasso()` qui prend au constructeur `B`, `lam` et `y` qui valent respectivement $B$, $\\lambda$ et $y$. Cette classe doit vous calculer $f(x)$, $\\nabla f(x)$ ainsi que la Hessienne dans les fonctions idoines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lasso() :\n",
    "    def __init__(self,B,lam,y) :\n",
    "        pass\n",
    "    def zeros(self) :\n",
    "        pass\n",
    "    def value(self,x) :\n",
    "        pass\n",
    "    def grad(self,x) :\n",
    "        pass\n",
    "    def Hess(self,x) :\n",
    "        pass\n",
    "f=lasso(B,lam,y)\n",
    "np.random.seed(42)\n",
    "x=np.random.randn(2*p)\n",
    "d=np.random.randn(2*p)\n",
    "print('## TEST DE LA FONCTION##')\n",
    "print(f.value(x))                         # 3576.442886034992\n",
    "print(f.grad(x).shape,f.grad(x)[3:6])     # (120,) [ 247.688236   -146.62731787  -37.53567201]\n",
    "print(f.Hess(x).shape,f.Hess(x)[15][3:6]) # (120, 120) [4.15670644 6.06401088 6.55285271]\n",
    "print('## TEST DE DERIVEE NUMERIQUE##')\n",
    "opt.deriv_num(f,x,d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez les classes `lin_cst` et `lasso` définies au dessus pour résoudre le problème du Lasso avec une méthode de Newton, une précision de $10^{-8}$, en partant du point $x_0=1$ pour les différentes valeurs de `t` données dans le tableau `tlist` ci-dessous. Vous afficherez pour chaque `t` le vecteur $w_0$ et sa reconstruction en utilisant le module `plt.stem` donné plus haut dans le notebook. Que remarquez vous sur la solution ? Pouvez-vous l'expliquer ?\n",
    "<div class=\"alert alert-block alert-info\"> VOTRE REPONSE CI-DESSOUS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlist = np.array([1, 10, 100, 1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez ci-dessous la fonction `central_path`. Vous mettrez une tolérance de $10^{-5}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les différentes valeurs de `lam_list` donnez ci-dessous, lancez l'algorithme d'optimisation `central_path` pour résoudre le problème du Lasso. vous mettrez une tolérance de $10^{-4}$. Que remarquez-vous quand $\\lambda$ var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_list = np.array([0.01*lam, 0.1*lam, lam,4*lam])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
