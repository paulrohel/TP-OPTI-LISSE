{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Méthode de point intérieur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de ce TP est d'utiliser des méthodes de point intérieur pour minimiser une fonction sous contrainte.\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\umin}[1]{\\underset{#1}{\\min}\\;}$\n",
    "$\\DeclareMathOperator{\\eqdef}{\\overset{\\tiny def}{=}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit $f$ une fonction régulière de $\\mathbb{R}^n$ dans $\\mathbb{R}$, nous nous intéresserons à deux types de problème : \n",
    "1. Le problème à contrainte linéaire \n",
    "$$\n",
    "    (\\mathcal{P}_{\\infty,\\text{ lin.}}) \\qquad \\umin{x\\in \\mathbb{R}^n, A x \\leq b} f(x)\n",
    "$$\n",
    "où $A$ est une matrice $\\mathcal M_{pn}(\\mathbb{R})$ et $b\\in \\mathbb{R}^p$.\n",
    "2. Le problème avec **une** contrainte non linéaire\n",
    "$$\n",
    "    (\\mathcal{P}_{\\infty,\\text{ non lin.}}) \\qquad \\umin{x\\in \\mathbb{R}^n, g(x)\\le 0} f(x)\n",
    "$$\n",
    "Où $g$ est une fonction régulière de $\\mathbb{R}^n$ dans $\\mathbb{R}$.\n",
    "\n",
    "\n",
    "L'idée des méthodes de point intérieur est d'approximer les problèmes $(\\mathcal P_{\\infty,\\bullet})$ en utilisant des [fonctions barrières logarithmiques](https://en.wikipedia.org/wiki/Barrier_function)\n",
    "\n",
    "$$\n",
    "\t(\\mathcal{P}_{t, \\text{non-lin.}}) \\qquad \\umin{x\\in \\mathbb{R}^d, } f_t(x) \\eqdef f(x) - \\frac{1}{t} \\text{ln}( -g(x)  )\n",
    "$$\n",
    "ou \n",
    "$$\n",
    "\t(\\mathcal{P}_{t, \\text{ lin.}}) \\qquad \\umin{x\\in \\mathbb{R}^d, } f_t(x) \\eqdef f(x) - \\frac{1}{t} \\text{Log}( b-A x  )\n",
    "$$\n",
    "avec\n",
    "$$\n",
    "\t\\text{Log}(u) \\eqdef \\sum_i \\ln(u_i)\n",
    "$$\n",
    "\n",
    "Ainsi la fonction -Log ou -log qui est une fonction strictement concave, agit comme une barrière pour la contrainte. On s'attend à ce qu'à la limite, quand $t$ tend vers $+\\infty$, le problème $(\\mathcal P_{t,\\bullet})$ tende vers le problème $(\\mathcal P_{\\infty,\\bullet})$.\n",
    "## I-Contrainte non-linéaire\n",
    "On suppose dans cette section que l'on se place dans le cadre d'une seule contrainte non-linéaire.\n",
    "Si $f$ et $g$ sont données et $f_t=f - \\frac{1}{t} \\text{log}( -g)$, \n",
    "  \n",
    "<div class=\"alert alert-block alert-info\"> Calculez ci-dessous le gradient et la Hessienne de $f_t$ en fonction des gradients et de la Hessienne de $f$ et de $g$ \n",
    "$$\\nabla f_t(x) = ?? \\quad H [f_t](x) = ?? $$\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "f_t'(x)\\cdot v \n",
    "    &= f'(x)\\cdot v - \\frac{1}{t} \\ln'(-g(x)) \\, (-g'(x)\\cdot v) \\\\[4pt]\n",
    "    &= f'(x)\\cdot v - \\frac{1}{t} \\frac{g'(x)\\cdot v}{g(x)} \\\\[4pt]\n",
    "    &= \\langle \\nabla f(x), v \\rangle \n",
    "       + \\left\\langle \\frac{1}{t g(x)} \\nabla g(x), v \\right\\rangle \\\\[6pt]\n",
    "    &= \\left\\langle \\nabla f(x) + \\frac{1}{t g(x)} \\nabla g(x), v \\right\\rangle\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "donc, par identification :\n",
    "$\\nabla f_t(x) = \\nabla f(x) + \\frac{1}{t g(x)} \\nabla g(x)$\n",
    "\n",
    "\\begin{aligned}\n",
    "f_t''(x)(u,v) \n",
    "    &= (f'(x)\\cdot v)' \\cdot u \\\\[4pt]\n",
    "    &= f''(x)(u,v)\n",
    "       - \\frac{1}{t} \n",
    "         \\frac{g''(x)(u,v) g(x) - g'(x)\\cdot v \\, g'(x)\\cdot u}{g(x)^2} \\\\[6pt]\n",
    "    &= v^T \\nabla^2 f(x) u\n",
    "       - \\frac{1}{t g(x)^2} \n",
    "         \\big( g(x) \\, v^T \\nabla^2 g(x) u \n",
    "               - \\langle \\nabla g(x), v \\rangle \n",
    "                 \\langle \\nabla g(x), u \\rangle \\big) \\\\[6pt]\n",
    "    &= v^T \\nabla^2 f(x) u\n",
    "       - \\frac{1}{t}\n",
    "         \\left(\n",
    "           \\frac{v^T \\nabla^2 g(x) u}{g(x)} \n",
    "           - \\frac{v^T \\nabla g(x)\\nabla^T g(x) u}{g(x)^2}\n",
    "         \\right) \\\\[6pt]\n",
    "    &= v^T \\left(\n",
    "           \\nabla^2 f(x)\n",
    "           + \\frac{1}{t}\n",
    "             \\left(\n",
    "               -\\frac{\\nabla^2 g(x)}{g(x)}\n",
    "               + \\frac{\\nabla g(x)\\nabla^T g(x)}{g(x)^2}\n",
    "             \\right)\n",
    "         \\right) u\n",
    "\\end{aligned}\n",
    "\n",
    "donc, par identification :\n",
    "$\\nabla^2 f_t(x)\n",
    "= \\nabla^2 f(x)\n",
    "  + \\frac{1}{t}\n",
    "    \\left(\n",
    "      -\\frac{\\nabla^2 g(x)}{g(x)}\n",
    "      + \\frac{\\nabla g(x)\\nabla^T g(x)}{g(x)^2}\n",
    "    \\right)$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On suppose que `f` et `g` sont des fonctions définies avec une classe dans le fichier `functions.py` (ainsi les calculs du gradient ou de la Hessienne de `f` et de `g` sont déjà faits). Créer ci-dessous une classe `non_lin_cst` qui calcule `f_t`, son gradient ou sa Hessienne. La classe prend à la construction la fonction `f`, la fonction `g` et la valeur de `t`. Le constructeur de la classe est déjà implémenté.\n",
    "Attention, dans le cas où `g(x)` est $>0$, la fonction `value(self,x)` doit rendre `np.inf` et ne doit pas rendre une erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import functions as func\n",
    "import Optim as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class non_lin_cst() :\n",
    "    def __init__(self,f,g,t) :\n",
    "        self.zeros()\n",
    "        self.f=f\n",
    "        self.g=g\n",
    "        self.t=t\n",
    "        self.nb_constraints=1. # nombre de contraintes du problème\n",
    "    def zeros(self) :\n",
    "        self.nb_eval=0 # number of evaluations of the function self.value()\n",
    "        self.nb_grad=0 # number of evaluations of the function self.grad()\n",
    "        self.nb_hess=0 # number of evaluations of the function self.Hess()\n",
    "    def value(self,x) :\n",
    "        self.nb_eval += 1\n",
    "        if self.g.value(x) > 0:\n",
    "            return np.inf\n",
    "        return self.f.value(x) - (1/self.t)*np.log(-self.g.value(x))\n",
    "    def grad(self,x) :\n",
    "        if self.g.value(x) > 0:\n",
    "            return np.full_like(x, np.inf)\n",
    "        self.nb_grad += 1\n",
    "        return self.f.grad(x) - (1/self.t)*self.g.grad(x)/self.g.value(x)\n",
    "    def Hess(self,x) :\n",
    "        self.nb_hess += 1\n",
    "        if self.g.value(x) > 0:\n",
    "            return np.full((len(x), len(x)), np.inf) \n",
    "        grad_g = self.g.grad(x).reshape(-1,1)\n",
    "        return self.f.Hess(x) + (1/self.t)*(-self.g.Hess(x)/self.g.value(x) +  grad_g@grad_g.T  /self.g.value(x)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de tester notre classe, on utilise comme `f` la fonction Rosenbrock et comme `g` la fonction `square2` définie ci-dessous :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doit être 1.0= 1.0\n",
      "Doit être 1.5012148269226977= 1.5012148269226977\n",
      "Doit être inf = inf\n",
      "######## TEST DE DERIVEE NUMERIQUE\n",
      "eps 1.0e-01 grad 3.2e-01 ratio 2.4e-03 angle 1.7e-04\n",
      "eps 1.0e-02 grad 3.2e-02 ratio 1.3e-04 angle 1.7e-06\n",
      "eps 1.0e-03 grad 3.2e-03 ratio 1.2e-05 angle 1.7e-08\n",
      "eps 1.0e-04 grad 3.2e-04 ratio 1.2e-06 angle 1.7e-10\n",
      "eps 1.0e-05 grad 3.2e-05 ratio 1.2e-07 angle 1.7e-12\n",
      "eps 1.0e-06 grad 3.2e-06 ratio 1.2e-08 angle 1.7e-14\n",
      "eps 1.0e-07 grad 3.2e-07 ratio 8.8e-10 angle 2.2e-16\n",
      "eps 1.0e-08 grad 3.1e-08 ratio 3.7e-09 angle 2.2e-16\n",
      "eps 1.0e-09 grad 6.9e-10 ratio 1.5e-08 angle 1.1e-16\n",
      "eps 1.0e-10 grad 1.8e-07 ratio 5.2e-08 angle 6.9e-15\n",
      "eps 1.0e-11 grad 3.7e-06 ratio 2.5e-06 angle 1.1e-13\n",
      "eps 1.0e-12 grad 5.1e-06 ratio 2.8e-05 angle 2.4e-12\n"
     ]
    }
   ],
   "source": [
    "class square2() :\n",
    "    def __init__(self) :\n",
    "        self.zeros()\n",
    "    def zeros(self) :\n",
    "        self.nb_eval=0 # number of evaluations of the function self.value()\n",
    "        self.nb_grad=0 # number of evaluations of the function self.grad()\n",
    "        self.nb_hess=0 # number of evaluations of the function self.Hess()\n",
    "    def value(self,x) :\n",
    "        # returns the value of the function at point x\n",
    "        self.nb_eval+=1\n",
    "        return 0.5*x[0]**2+7/2.*x[1]**2-1\n",
    "    def grad(self,x) :\n",
    "        # returns the gradient of the function at point x\n",
    "        self.nb_grad+=1\n",
    "        return np.array([x[0],7*x[1]])\n",
    "    def Hess(self,x) :\n",
    "        # returns the Hessian of the function at point x\n",
    "        self.nb_hess+=1\n",
    "        to_return=np.zeros((2,2))\n",
    "        to_return[0,0]=1\n",
    "        to_return[1,1]=7\n",
    "        return to_return\n",
    "f_t=non_lin_cst(func.Rosen(),square2(),0.33)\n",
    "x_0=np.array([0.,0.])\n",
    "print('Doit être 1.0=',f_t.value(x_0))\n",
    "x_0=np.array([0.2,0.12])\n",
    "print('Doit être 1.5012148269226977=',f_t.value(x_0))\n",
    "x_0=np.array([2,1.3])\n",
    "print('Doit être inf =',f_t.value(x_0))\n",
    "print('######## TEST DE DERIVEE NUMERIQUE')\n",
    "a=np.array([0.2,0.12])\n",
    "d=np.random.randn(2)\n",
    "opt.deriv_num(f_t,a,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme d'optimisation\n",
    "Vous devez récupérer votre algorithme de Newton ainsi que la recherche linéaire de Wolfe (avec un step initial de $1$ ) dans la cellule ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_wolfe(x,function,step,descent,f,df) :\n",
    "    e1 = 1e-4\n",
    "    e2 = 0.9\n",
    "    sl = 0\n",
    "    sh = np.inf\n",
    "    step2 = step\n",
    "    while True:\n",
    "        x2 = step2*descent + x\n",
    "        f2 = function.value(x2)\n",
    "        if f2 > f + e1*step2*np.dot(df,descent):\n",
    "            sh = step2\n",
    "            step2 = (sl + sh)/2\n",
    "        else:\n",
    "            df2 = function.grad(x2)\n",
    "            if np.dot(df2,descent) < e2*np.dot(df,descent):\n",
    "                sl = step2\n",
    "                if sh < np.inf:\n",
    "                    step2 = (sl + sh)/2\n",
    "                else:\n",
    "                    step2 = 2*sl\n",
    "            else:\n",
    "                return x2,f2,df2,step2\n",
    "    \n",
    "def ls_wolfe_step_is_one(x,function,step,descent,f,df) :\n",
    "    return ls_wolfe(x,function,1.,descent,f,df)\n",
    "\n",
    "    \n",
    "def dc_Newton(x,function,df) :\n",
    "    descent = np.matmul(np.linalg.inv(function.Hess(x)),-df)\n",
    "    c = np.dot(descent,-df)/(np.linalg.norm(descent)*np.linalg.norm(df))\n",
    "    if c > 0.1:\n",
    "        return descent\n",
    "    else:\n",
    "        return -df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant lancer une optimisation en utilisant la méthode de Newton avec un pas de Wolfe initialisé à $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=   0 f=1.000e+00 df=2.000e+00 comp=[   1,   1,   0]\n",
      "iter=   1 f=8.586e-01 df=8.192e+00 comp=[   3,   2,   1]\n",
      "iter=   2 f=7.028e-01 df=3.443e-01 comp=[   4,   3,   2]\n",
      "iter=   3 f=6.973e-01 df=2.196e-01 comp=[   5,   4,   3]\n",
      "iter=   4 f=6.972e-01 df=1.166e-03 comp=[   6,   5,   4]\n",
      "iter=   5 f=6.972e-01 df=6.426e-07 comp=[   7,   6,   5]\n",
      "iter=   6 f=6.972e-01 df=3.808e-14 comp=[   8,   7,   6]\n",
      "Success !!! Algorithm converged !!!\n",
      "x final= [0.25636244 0.05915168]\n"
     ]
    }
   ],
   "source": [
    "f_t=non_lin_cst(func.Rosen(),square2(),0.33)\n",
    "res=opt.main_algorithm(f_t,5,np.array([0,0]),dc_Newton,ls_wolfe_step_is_one,tol=1.e-7,verbose=True)\n",
    "final_x=res['list_x'][-1]\n",
    "print('x final=',final_x)\n",
    "# VOUS DEVEZ TROUVER\n",
    "#Fonction de Rosenbrock\n",
    "#Fonction (x,y) --> x^2/2+7/2*y^2-1\n",
    "#Fonction Interior point method\n",
    "#iter=   0 f=1.000e+00 df=2.000e+00 comp=[   1,   1,   0]\n",
    "#iter=   1 f=8.586e-01 df=8.192e+00 comp=[   3,   2,   1]\n",
    "#iter=   2 f=7.028e-01 df=3.443e-01 comp=[   4,   3,   2]\n",
    "#iter=   3 f=6.973e-01 df=2.196e-01 comp=[   5,   4,   3]\n",
    "#iter=   4 f=6.972e-01 df=1.166e-03 comp=[   6,   5,   4]\n",
    "#iter=   5 f=6.972e-01 df=6.426e-07 comp=[   7,   6,   5]\n",
    "#iter=   6 f=6.972e-01 df=3.988e-14 comp=[   8,   7,   6]\n",
    "#Success !!! Algorithm converged !!!\n",
    "#x final= [0.25636244 0.05915168]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La méthode du chemin central\n",
    "Notre objectif est d'envoyer le paramètre $t$ vers $+\\infty$. Pour ce faire on va suivre la méthode du chemin central, nous commençons par nous donner $t_1$ et nous notons $x_0$ le point initial et $x_1$ le minimiseur donné de $f_{t_1}$ par une méthode de Newton commençant à $x_0$. Ensuite nous multiplions $t_1$ par $\\mu$ pour obtenir $t_2$ et nous lançons une méthode de Newton commençant à $x_1$. On note $x_2$ le résultat obtenu. \n",
    "Ainsi nous allons construire une suite de points solutions du problème \n",
    "$$x_k \\in \\textrm{argmin} f_{t_k}(x).$$\n",
    "avec $t_{k}=\\mu t_{k-1}$ et $x_{k}$ une solution donnée par un algorithme de Newton (avec recherche de Wolfe) et avec comme point de départ $x_{k-1}$. L'idée d'utiliser comme initialisation la solution d'un problème d'optimisation s'appelle \"warm restart\".\n",
    "\n",
    "On peut montrer que l'erreur que l'on fait entre $x_k$ et $x^\\star$ (où $x^\\star$ est le minimum de $f$) vérifie :\n",
    "$$ 0 \\le f(x_k)-f(x^\\star)\\le \\frac{p}{t_k},$$\n",
    "où $p$ est le nombre de contraintes. Ainsi on va s'arrêter dès que $\\varepsilon t_k > p$ où $\\varepsilon$ est la précision de l'algorithme de Newton.\n",
    "Ecrire une fonction `def central_path(function,mu,varepsilon,x0)` qui implémente cette stratégie. Elle prend en argument \n",
    " 1. `function`: une instance de la classe `non_lin_cst`\n",
    " 2. `mu`: un réel $>1$ qui est le facteur multiplicatif de `t`\n",
    " 3. `varepsilon`: une précision\n",
    " 4. `x0`: un point de départ de la méthode\n",
    " \n",
    "Cette fonction doit rendre `costs,x` où\n",
    "\n",
    "1. `costs` est la liste des coûts pour toutes les itérations\n",
    "2. `x` est le point final d'arrivée\n",
    "\n",
    "On rappelle que la valeur de $p$ se trouve dans `function.nb_constraints` tandis que la valeur de $t$ se trouve dans `function.t`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimal value  0.09946170477767807\n",
      "x [0.6852544  0.46758137]\n"
     ]
    }
   ],
   "source": [
    "def central_path(function,mu,varepsilon,x0) :\n",
    "    x = x0\n",
    "    costs = []\n",
    "    while varepsilon*function.t <= function.nb_constraints:\n",
    "        res = opt.main_algorithm(function,5,x,dc_Newton,ls_wolfe_step_is_one,tol=1.e-4,verbose=False)\n",
    "        x=res['final_x']\n",
    "        function.t = mu*function.t\n",
    "        costs += res['list_costs']\n",
    "    return costs, x\n",
    "\n",
    "f_t=non_lin_cst(func.Rosen(),square2(),10)\n",
    "costs,x=central_path(f_t,10,1.e-9,np.array([0,0]))\n",
    "cost_min=np.min(costs)\n",
    "print('minimal value ',cost_min) # minimal value  0.0994618583079908\n",
    "print('x',x) # x [0.68525439 0.46758135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAASd5JREFUeJzt3Ql8VNXB/vEnewgk7AQCYV8EIvsiKAoqWFAUrRWXKlp43/LSqoDWgvat4r+K1ZbSvgqVolK1KrUItUoFVDZFlB1ZZA2EPSQBskHW+X/OCUmzk+AkM3Pn9+3ndmbuDDdnTu5kHs9yT4DL5XIJAADAIQI9XQAAAAB3ItwAAABHIdwAAABHIdwAAABHIdwAAABHIdwAAABHIdwAAABHIdwAAABHIdwAAABHIdwAHrJgwQIFBARUuK1atarGfnbbtm314IMPXta/feeddzR79uxynzPlfuaZZ+TNPvvsM/Xr109169a15V2yZEm5rzt+/Lh9L1u3bi3znKm7evXqyWmWLl3q9b8/oCqCq/QqADXmjTfe0BVXXFFmf7du3byy1k242bFjhyZPnlzmua+++kqtWrWStzKrzdx1113q3LmzPvzwQxtwunTpUmG4mTFjhg2CvXr1kj8w4eaVV14h4MDnEW4AD4uLi7MtCU5w1VVXyZuZwJKSkqLbb79dN9xwg6eLA6CG0C0FeLnevXtryJAhZfbn5eWpZcuWuuOOO4r2mS/uSZMm2f2hoaFq3769nnrqKWVlZVWpi+zQoUMl9puuseJdZEOHDtXHH3+sw4cPl+hCq6xbyrTy3HbbbWrYsKHCw8NtK8hf//rXcn/Ou+++a8sbExOjqKgo3XjjjdqzZ0+V6umLL76wgSUyMlIREREaPHiwLWshU67CVqVf/vKX9ueZVpnymPL079/f3n/ooYeK3mfp97Z//36NGjXKdlHFxsbqscceK1PX2dnZ+s1vfmNb58LCwtS0aVN7zNOnT1fpfX399dcaPXq0GjdubOuvQ4cOZVrNLvXejczMTD3++ONq166dPU6jRo1sqDZ1XtjVZlptjOK/29LnBOALCDeAh5mQkpubW2Iz+wqZL0Lz5bVv374S/2758uW2JcI8b1y4cEHDhg3Tm2++qalTp9ovtx//+Md68cUXSwSg72POnDm6+uqr1bx5c9sFVbhVxAQT80W7c+dO/elPf9IHH3xgu9vMF6kpV2lPPvmkDU7z58/XvHnz7Hs2X+zF66M8q1ev1vXXX69z587ptddes1/Y5ove/NuFCxfa10yYMMH+fOPhhx+25V68eHG5x+vTp4/tLjR+9atfFb1Pc4xCOTk5uvXWW22o+Oc//6mf/OQn+sMf/qDf/va3Ra/Jz8+3we6FF17Qvffea38n5v6KFStsUDx//nyl72vZsmU22CYkJGjWrFn697//bctz6tSpar13w5wTc+fO1SOPPKJPPvlEb731ln70ox8pOTnZPv+///u/uvPOO+394r/bFi1aVFpGwCu5AHjEG2+84TIfwfK2oKCgotclJSW5QkNDXU8++WSJf3/XXXe5oqOjXTk5Ofbxn//8Z/tv//73v5d43W9/+1u7f/ny5UX72rRp4xo3blyZssTHx5f4tytXrrT7zW2hm2++2f778pjXPv3000WP7777bldYWJgrISGhxOtGjhzpioiIcJ09e7bEzxk1alSJ15n3YvZ/9dVXldblVVdd5WrWrJkrLS2taF9ubq4rLi7O1apVK1d+fr7dZ96fOd5LL73kupQNGzbY15q6Kc3UXXl1bcrfpUuXosfvvvuufd2iRYvKPfacOXMqLUOHDh3sdv78+e/93s3jMWPGVPrzfvazn9lyAb6OlhvAw0xLy4YNG0pspiuikOmOMP8VbrpyTEuAcebMGdta8MADDyg4uGDo3Oeff24HyBb+13ehwllRZpZQbTNlMi0bpsumdJlMN0npVh/TElJcjx497K1pzalIRkaGrS/zvovPYAoKCtL999+vo0ePVrlrqzpMl435vZQub/GyfvTRR2rQoIF9XfGWOdM1Z1q/KpsRt3fvXh04cEDjx4+33Ujf970PGDDAtvxMmzbN/txLtRoBvowBxYCHde3a9ZIDik2Xx6JFi2x3xk033WS7HszYjuLTuU33gvnCLD4GxmjWrJkNQIXdD7XJ/MzyujXMmJrC54szQa44M0bFqOyL2AQ902hUnZ/jDmZsS+nQYcprugcLme6js2fP2vFP5UlKSqrw+IVjciqbfVad9266Bc2xTFeV6TozZTfn0ksvvaROnTpd8v0CvoRwA/gA8yVkvqzMOBBz39wOHDiwxHRxEwzMf8WbL7viAScxMdG2FjRp0qTC4xd+SZceDFvZl29VmDKdOHGizH4zVsiorExVZQYqBwYG1vjPuRzm55o6MGNcymPGxlTEDDw2TOuLO967adUzU9vNZkJXYSuOaVX67rvvqv3eAG9GtxTgAwq7GcwF59auXauNGzfa1pziTPdPenp6mYvSmW6vwucrUjhraPv27SX2m2vBlGZaJ6rapWF+pumaKvyiLV4m0/Lhjqnj5kvbBD0zWLh4uUwX3ttvv21bK8x1baqrKq1Gl3LLLbfYlhMzINq0zpXeKrrGjmHKbGZGvf766xXOdrvc9x4dHW1b/e655x7bbWW6CN31ngFvQMsN4GFmqrRpWSnNfLEV/te7YcKM6U4ws27q1KmjsWPHlni9GX9jpvKOGzfOTt+98sor7Syr559/3k5XNtOqK2KmPZsvWjNV2JTFtAiYmUTm35dmjmu+TM3Mm759+9qWg4q61Z5++mk77sTM4vr1r39tpx//7W9/s7OGzGyp+vXryx1mzpyp4cOH259j3oPpBjIzu0zdmi680l11VWHq39SzKa/pOjRjWkzrWWF3T1Xcfffd9t+b+n/00UftuJeQkBDbGrNy5Uo7k8pcc6ci5vdpWlZMCJwyZYpat25tZ06ZWVTmuNV57yYEmbBlxgWZ3+/u3bvtjKlBgwbZoFn4uzXMeTZy5Egbqs3rK+pWA7yWp0c0A/6qstlSZvvLX/5S5t8MHjzYPnffffeVe8zk5GTXxIkTXS1atHAFBwfbWU3Tp093XbhwocTrSs+WMvbu3esaMWKEKyoqytW0aVPXww8/7Pr444/LzJZKSUlx3Xnnna4GDRq4AgICSsyuKT1byvj2229do0ePdtWvX9/O+urZs2eZGUiFs6Xef//9EvsLZzeVN2OptLVr17quv/56V926dV116tSxs4j+9a9/lXu8qsyWKpztdMUVV7hCQkJKvDdTd+bnlGaeL/1n1cxm+93vfmffd3h4uKtevXr2mD/96U9d+/btu2QZzEwxM7vM1J+ZeWZmT02ZMqXa733atGmufv36uRo2bGiP0759e3scMxuvUFZWlmvChAn291/4uy09gw7wBQHm/zwdsAAAANyFMTcAAMBRCDcAAMBRCDcAAMBRCDcAAMBRCDcAAMBRCDcAAMBR/OYifuaKneYqqeZy55dzQS8AAFD7zBVr0tLS7AU0zUVDq8Jvwo0JNqVXJgYAAL7hyJEjlS4k65fhpnCBOlM5UVFRbj12Tk6Oli9frhEjRthLq4N6qymca9RbbeJ8o9684VxLTU21jROVLTTrt+GmsCvKBJuaCDdmbRZzXMIN9VaTONeot9rE+Ua9edO5Vp0hJQwoBgAAjkK4AQAAjkK4AQAAjkK4AQAAjkK4AQAAjkK4AQAAjkK4AQAAjuJT4eajjz5Sly5d1KlTJ82fP9/TxQEAAF7IZy7il5ubq6lTp2rlypX2Ij99+vTRHXfcoUaNGnm6aAAAwIv4TMvNN998o+7du6tly5b2EsyjRo3SsmXLPF0sAADgr+FmzZo1Gj16tF3V01xCecmSJWVeM2fOHLVr107h4eHq27ev1q5dW2LhSxNsCpnFs44dO1ZbxQcAAD6i1sJNRkaGevbsqZdffrnc5xcuXKjJkyfrqaee0pYtWzRkyBCNHDlSCQkJRUuef591JgAAgH+otTE3JqiYrSKzZs3S+PHjNWHCBPt49uzZtttp7ty5mjlzpm21Kd5Sc/ToUQ0cOLDC42VlZdmt+KqihYtzmc2dtiWkKCuv4NiousL6ot6os5rGuUa91SbON/fW2eV8RwS4ymsSqWGmxWXx4sUaM2aMfZydnW1XA33//fd1++23F73u0Ucf1datW7V69Wo7oLhr165atWpV0YDi9evXq3HjxuX+jGeeeUYzZswos/+dd96xP8td0nOkpzYWZMQGoS5F1zGbStxGhpj37LYfCQCA38jMzNS9996rc+fO2e9/n5ktlZSUpLy8PEVHR5fYbx6fPHnS3g8ODtbvf/97DRs2TPn5+XriiScqDDbG9OnT7eyq4i03sbGxGjFiRJUrpyq+O5mmxrs3KjkjR2ezA+y251zJ19SvE6z/uqad/ntIW7rSSqXxFStWaPjw4RUucY+SqLPLQ71Rb7WJ8829dVbY81IdXhFuKhpDYxqViu+79dZb7VYVYWFhdivNVJo7v0ivjG2k9dOG6f1/LlWH3oN1KOW8DpzO0P7EdB04na4jKZk6dz5Xv1uxT5k5+frFTV0IODX8O/EH1Bn1xvnm/ficuqfOLuf7wSvCTZMmTRQUFFTUSlMoMTGxTGuOt6obIvVp3UADOzQtsf9CTp7eXn9Yv/l4t+asOqDcfJemj7yCgAMAgJOvcxMaGmqnfpsmqeLM48GDB8uXhYcEacKQ9nr2tu728bw1B/XsR7vKnf0FAAC+v1pruUlPT9f+/fuLHsfHx9vBwuYKw61bt7bjY+6//37169dPgwYN0rx58+w08IkTJ8oJHhjUVsGBgXpy8bd648tDyst36ZnR3RUYyEhjAAB8Mtxs3LjRDgYuVDjYd9y4cVqwYIHGjh2r5ORkPfvsszpx4oTi4uK0dOlStWnTRk5x78DWCg4M0C8/2K43vzqsnDyXnhsTR8ABAMAXw83QoUMv2RUzadIkuznZXf1jFRQYoF/8Y5ve/SZBefn5mnlHD7sPAAA4ZMyNv/lh31b6w9heMnnm7xuP6hfvb1N2br6niwUAgCN4xWwpf3Rbr5a2tebR97bqgy3HtGrvad3eu6Xu6herLs0jPV08AAB8FuHGg27pEaPQoEA9tWSHTqdl6bUv4u3Ws1V9/ahfrEb3jFH9Olz/BQCA6iDceNiI7s11/RXNtHrvab2/8ag+3X1K246es9v/+2iXfhDXXPcMaK2r2ld8NWYAAPAfhBsvEBwUqBu6RtstKT1LS7Ycs0Fnz6k0/XPrcbvd3T9WT4/urjqhQZ4uLgAAXo0BxV6mSb0we9G/TyYP0Yc/v1r3DIi1i26+t+GIbnvlC+07lebpIgIA4NUIN17KrKnVo1UDO0387fED1TQyTHtPpWv0y1/o7xuPcIVjAAAqQLjxAVd3bKKljwzRkE5NdCEnX0/8Y7umLNyq9KxcTxcNAACvQ7jxEabl5q8PDdATP+hip5Av2Xpco//vC+08fs7TRQMAwKswoNiHmHWoJg3tqAFtG+nhd7coPilDt89Zp/8a0k7dWtRXm8YRat04QlHhTB8HAPgvwo0P6te2ke2mMks4fLo7Ua+sPFDi+YYRIWrduK7aNIqwgadx3VBFhoeoXniwIs0WFmJvzeN6YcFlln4IKGf8T0XMP63seQAAahvhxkc1rBuqvzzQT//YdFTrDiTrcHKGElIylZSerTOZOTqTeVbbjpyt8XJ0aFpXr97fVx2bcVVlAIB3INz4MNNiYq5kbLZCZpBxQnKmElIydNjeZupsZo7SsnKVdiFH6RfMba59nTsGJB84naG7563X3yZcxbIRAACvQLhxGNPN1C0mym6XkpfvUmZ2rvKLL9ZeauF2V+kdxZhw9NO3Nmnn8VTd85f1dsp6VX4uAAA1idlSfsyMtTFjccz6VUVbRMmtQURohVurhhF6Z8JVdi2slIxsG3C+PcrsLQCAZxFu8L2YAPTWhIHq3bqBzp3P0b3z12tLwhlqFQDgMYQbfG9m6vmbPxmg/m0b2vE897/2jTYeSqFmAQAeQbiBW5jurQUPDdBV7RvZsTgPvP6Nvj6YTO0CAGod4QZuUzcsWG88OEDXdGyizOw8PfjGBn25P4kaBgDUKsIN3KpOaJDmj+un6zo31fmcPD20YIP+/e0JahkAUGsIN3C78JAgzXugr27qHq3s3HxNemez3lp/mJoGANQKwg1qRFhwkObc11f3Dmwtl0v63yU79Pvle+QyDwAAqEGEG9TodXSeGxOnyTd2so//7/P9mv7Bt8rNy6fWAQA1hnCDGl8iYvKNnfXc7XF2kc33NhzRxLc363x2HjUPAKgRhBvUivsGttHcH/dVaHCgPt19Sj9+7Wu75hUAAO5GuEGtual7c/1twkBFhQdr0+Ezumf+NzqQKjvoGAAAd2HhTNSq/m0b6f2JgzXu9W+0/3SG/nQ6WK/u+Vx9WjfUgHaN7GaWcogI5dQEAFwevkFQ67o0j9SiSYP1/Me7tGr3CWXk5uurg8l2sydlYICubFVfvWMbqk5o5Y2LAQqo8DkTkm7oGu328gMAvBvhBh7RskEdzb6rhz7++Ki69L9Wm4+makN8ir6OT9GJcxe0JeGs3b4PM4B50f8MVu/WDd1WbgCA9yPcwKMCAqSOzeqpa8uGdtCxuQ7O0TPnteFQinYdT1VeseviVOcSOTuPn9OGQ2f0xD+266NHrrHX3QEA+AfCDbxu6nhsowi73dHn8o9zJiNbw/+wWvsS0/Xy5/v12Igu7iwmAMCLMVsKjtSwbqj+321x9v6cVQe049g5TxcJAFBLCDdwrJFXttCoK5srL99lu6dyuDIyAPgFwg0cbcatcWoQEaJdJ1I1b81BTxcHAFALCDdwtKaRYXp6dDd7/4+f7tO+U2meLhIAoIYRbuB4Y3q11LAuTZWdl69f/GO77aYCADgX4QZ+MQPr+TuuVGRYsLYeOas3voz3dJEAADXIZ8LNkSNHNHToUHXr1k09evTQ+++/7+kiwYe0qF9HT97c1d7/3fI9OpSU4ekiAQD8PdwEBwdr9uzZ2rVrlz799FNNmTJFGRl8QaHq7u4fq8EdGutCTr5+uWi78umeAgBH8plw06JFC/Xq1cveb9asmRo1aqSUlBRPFws+1j31wh09VCckyC7z8Ma6Q54uEgDAm8PNmjVrNHr0aMXExNgvkSVLlpR5zZw5c9SuXTuFh4erb9++Wrt27WX9rI0bNyo/P1+xsbFuKDn8SevGEXriBwVXK/5/H+3SrBV7acEBAIdx2/ILpouoZ8+eeuihh/TDH/6wzPMLFy7U5MmTbcC5+uqr9eqrr2rkyJG2m6l169b2NSbwZGVllfm3y5cvt6HJSE5O1gMPPKD58+dXWh5znOLHSk1Ntbc5OTl2c6fC47n7uE7nqXq7t19LHU3J0GtfHtafPtunvSdT9eIdcaoT6v3rT3GuUW+cb96Pz6l76+xyviMCXGalQjczLTeLFy/WmDFjivYNHDhQffr00dy5c4v2de3a1b5m5syZVTquCSvDhw/Xf/3Xf+n++++v9LXPPPOMZsyYUWb/O++8o4iIiGq9HzjT+sQA/f1goPJcAYqt69KELnlqEObpUgEAisvMzNS9996rc+fOKSoqSl4TbrKzs22gMDOcbr/99qLXPfroo9q6datWr159yWOaYpo316VLFxtcLqW8lhvTjZWUlFTlyqkqkypXrFhhg1dISIhbj+1k3lBvZuXwn727VWcyc9QsMkxz7+2lHq3qy1t5Q535IuqNeuN8893PqPn+btKkSbXCTa2sCm4CRV5enqKjo0vsN49PnjxZpWN8+eWXtmvLTAMvHM/z1ltv6corryz39WFhYXYrzVRaTX0p1OSxncyT9Ta4UzN9+PNrNP6vG7T3VLrufW2Dfvejnhrds6Ab1FtxrlFvnG/ej8+pe+rscr4faiXcFG/RKd0aU3pfRa655ho7iBhwt9hGEVr0P4P16Htb9fl3iXr43S3al5iuSUM7KDzE+8fhAAA8EG5Mc1JQUFCZVprExMQyrTmAJ0SGh+gvD/TTbz/5zi6waQYav7Jyv9o1qasrmkde3KLUpXmkWjWsU+VQDgCofbUSbkJDQ+1MKNOfVnzMjXl822231UYRgEsKCgzQk6O6qlOzenrh398pOSNb+xPT7fbR9hNFr6sXFqy2TSIUGhRo/03hFhgQoOBijwu2QLuv8LnAwIu3pbJRVcOSab08eSRQQy7kqBFdoABQs+EmPT1d+/fvL3ocHx9vBwubi+2Zqd5Tp061M5z69eunQYMGad68eUpISNDEiRPdVQTALX7UL1Z39m2lU6lZ2n0yVXtOpum7E6n67mSaDpxOV3pWrnYcK7i0gGcEqumK/Xrujh4eLAMA+EG4MRfWGzZsWNFjE2aMcePGacGCBRo7dqy9Rs2zzz6rEydOKC4uTkuXLlWbNm3cVQTAbUxLSvP64XYb1qVZ0f6cvHwdPJ2ho2cylZvvshcAtLcul3LzXMpzueyq44XP5RVuF/cXPqeLkxSLT1UsnLfoKrG3pHOZ2Xr76yP6+6aj+p9hHdWqIZc1AIAaCzdmUctLzSqfNGmS3QBfFRIUaMfdmM1T0yW/+e6w9p4L1P99tl+/vZPWGwDw2bWlABQYFVswa/Afm48qntXNAaAMwg3gY9pFSkM7N7FdXH/8dK+niwMAXodwA/igyTd0tLf/3HZce0+lebo4AOBVCDeAD+oeE6WRcc3tIOQ/rKD1BgCKI9wAPmrK8M4yl8f5946T2nHsnKeLAwBeg3AD+KjO0ZG69eIaWLNovQGAIoQbwIc9ekMneyVksybW5oQzni4OAHgFwg3gw9o3racf9mlp789aztgbADAIN4CPe/j6TgoJCtAX+5P01YFkTxcHADyOcAP4uNhGEbq7f2t7f9aKPZe8UjgAOB3hBnCAn1/fUWHBgdpw6IzW7EvydHEAwKMIN4ADREeF6/6rChah/d2yPbqQk+fpIgGAxxBuAIeYOLSDIkKD9O2xc7rupZV6a/1hZecWrEMFAP6EcAM4RJN6Yfq/e3orpn64TqVm6X+X7NCw363S3zccUW4eIQeA/yDcAA5yQ9dorfzFUD17W3c1iwzTsbPn9cSi7bpx1mot3nLULrYJAE5HuAEcJiw4SA8Maqs1TwzTr27uqkZ1Q3UoOVNTFm7TTbPXaOGGBCWmXvB0MQGgxgTX3KEBeFJ4SJAmDGmvewa01oJ1hzRvzUHtT0zXLxd9a5/v2iJK13Zuous6NVXftg1tKAIAJyDcAA5XNyxYPxvWUfcPaqO3vjqs5TtPavuxc9p9ItVur64+aAciD2rfWNd2bqrWjSNUJyTIbma/CUl1Qi/eDw5SYGCAp98SAFSKcAP4iajwEBtyzJaSka21+05r9d7TWrM3SUnpWfrsu0S7XYrJNmY9K7MFBwbax8FB5tbskwJUEH7MiuX29uK/CyjccZlCgwP1yx900Q/iWnyv4wBwPsIN4IfMOJzberW0W36+S7tPptqgY5ZvOJOZrfPZeQVbTsF2Iec/s63MmOT8PJdy8szg5NqdhfWbj3frxq7RNkwBQEUIN4CfM91M3WPq223S0I7lvsYEoAu5BYEnz+Wys65y8wpuiz/Ov7j0Q/EVIFz6z4PLXRnCHHfCXzfq6Jnz+veOkxrdM+byDgTALxBuAFQpAEWEBtvNU8wMsD98utcOjL6lR4vv3c0FwLlo2wXgE8yAaLN+lrkC89fxKZ4uDgAvRrgB4DPjhH7Ur5W9/5c1Bz1dHABejHADwGeMv6a9nYVlZnXtT0zzdHEAeCnCDQCf0a5JXY3oFm3vz18b7+niAPBShBsAPuW/r21vbz/YfEyJaSwjAaAswg0An9K3TSP1ad1A2Xn5enPdYU8XB4AXItwA8NnWm7fWH1Zmdq6niwPAyxBuAPic4d2aq23jCJ07n6P3Nx71dHEAeBnCDQCfY9a1Gj+koPVm/hcH7RWSAaAQ4QaAT7qzTys1jAjRkZTzWrbzpKeLA8CLEG4A+KQ6oUG6f1Bbe//VNQflutyFqwA4DuEGgM96YFAbhQYHatuRs9pw6IyniwPASxBuAPisJvXC9MM+BUsymAU1AcAg3ADwaROGtLO3n+4+pX9tO87gYgC+F24yMzPVpk0bPf74454uCgAv0KFpPY2Ma27vP/zuFg2ftVrvfpOgCzl5ni4aAA/xuXDz3HPPaeDAgZ4uBgAv8tKPeurnwzoqKjxYB5MyNP2DbzXkxZWas2q/vRYOAP/iU+Fm3759+u677zRq1ChPFwWAF6kXFqzHb+qiddNv0K9u7qoW9cN1Oi1LL36yR1e/8Lle+GSPzmR5upQAfC7crFmzRqNHj1ZMTIwCAgK0ZMmSMq+ZM2eO2rVrp/DwcPXt21dr166t1s8wXVEzZ850V5EBODDkTBjSXmueGKZZd/VUl+hIpWfl6rUvD+uZzcG68Q9f6LG/b7PdVvtOpSmfi/8BjhTsrgNlZGSoZ8+eeuihh/TDH/6wzPMLFy7U5MmTbcC5+uqr9eqrr2rkyJHatWuXWrdubV9jAk9WVtn/vFq+fLk2bNigzp07223dunXuKjYABwoJCtQdfVrp9t4ttWrPac1dtV8bDqXocEqm3RZtLliyoX6dEPVt09BurRrWsVc+Dgwo3AquhGzvBwYooNTPCCi1o+wr/qNpZJi6NI+sibcKoCbDjQkqZqvIrFmzNH78eE2YMME+nj17tpYtW6a5c+cWtcZs2rSpwn+/fv16vffee3r//feVnp6unJwcRUVF6de//nW5rzchqXhQSk1Ntbfm35nNnQqP5+7jOh31Rp3Vhms6NNTA1r30z3+vUMOOfbTteJo2J5zVtqPn7Hicz79LtFtNe3dCf/Vr01C+hM8o9eYN59rlfLcGuGrgsp6mW2rx4sUaM2aMfZydna2IiAgbTG6//fai1z366KPaunWrVq9eXa3jL1iwQDt27NDvfve7Cl/zzDPPaMaMGWX2v/POO7YsAPxbXr50NFOKTwvQobQApedILgXI/EXMl+yt+eNoeq4u1XtV2dMZOdK5nAD1bJSvn3QxRwZQ3VnS9957r86dO2cbNWq15aYySUlJysvLU3R0dIn95vHJkzWzJsz06dM1derUEi03sbGxGjFiRJUrp6pMqlyxYoWGDx+ukJAQtx7byag36swfzrW9p9J088tfacfZIPW+eqgd7Owr+IxSb95wrhX2vFRHrYSb4i06xZlGo9L7quLBBx+85GvCwsLsVpqptJr641aTx3Yy6o06c/K51r1VIw1s10hfx6fo/c3H9diILvI1fEapN0+ea5fzma2VqeBNmjRRUFBQmVaaxMTEMq05AOA04wYXLPBpZmll5XJxQaCm1Uq4CQ0NtTOhTJNTcebx4MGDa6MIAOAxw7tFKzoqTEnp2fpkR810xQOogXBjZjCZwcFmM+Lj4+39hIQE+9iMf5k/f75ef/117d69W1OmTLHPTZw40V1FAACvnZp+38A29v5f1x3ydHEAx3PbmJuNGzdq2LBhRY8LB/OOGzfOzm4aO3askpOT9eyzz+rEiROKi4vT0qVL7TpRAOB0dw+I1f99vs9OQ99x7JziWtb3dJEAx3JbuBk6dKgdIFyZSZMm2Q0A/E2zyHCNjGuhD7cd15tfHdKLd/b0dJEAx/KptaUAwJc9MKigpfqfW4/rbGa2p4sDOBbhBgBqiVnmoVuLKGXl5uvvG49Q70ANIdwAQC0x1/UqbL15e32C8li4E6gRhBsAqEW39WqpqPBgJaRkavXeml/TCvBHhBsAqEV1QoN0V79Ye//Nrw5T90ANINwAQC378VVtZFaeWbXntA4lZVD/gJsRbgCglrVtUlfXdW5q77+9ntYbwN0INwDgAeMGFaw3ZWZNnc9mvSnAnQg3AOABpuWmdaMIpV7I1T+3HuN3ALgR4QYAPCAwMED3X1UwLfzVNQf19cHkS17lHUDVEG4AwEN+1K+VnRYen5ShsfPWa/gf1ui1L+K5ejHwPRFuAMBDGkSE6oNJV+ueAbGKCA3S/sR0/b+PdmnA859p6sKt2nAohdYcwJMLZwIAqq9js3qaeUcPPTmqq11z6p2vE7TrRKo+2HLMbp2j62l4t2h1j6mv7jFRdpyOudIxgIoRbgDAC0SGh9jr39w3sLW2HT2nd74+bFcQ33sq3W5FrwsLVteYKBt0zDpVXVtEqW5YsEzcCQwIsNfPMeN5ij8uE4VK7Sh4dVm5uTlihQj4IsINAHgR0yrTK7aB3X51Szct3X5CW4+c1c7jqdpzMk1pWbn6Jj7FbrWhY1SQbh7FQGf4FsINAHipqPAQ3T2gtd2MnLx8Oy7HBJ2dx8/Z2wOJ6crOy5eZaJXvcl3cJBV7XJnKnjX/dH9qgLYeOacBHQouOgj4AsINAPiIkKBA2w1ltjv7tqrxnzdl4RYt3nJcCzcdJdzApzBbCgBQrrv7FQSoj789qdQLOdQSfAbhBgBQrt6x9dW8jksXcvL1zy1cRRm+g3ADAKhwcPPg6Hx7/29fJ3DNHfgMwg0AoEL9mrgUFhyo706m2SnqgC8g3AAAKlQ3RPpB92h7/92vE6gp+ATCDQCgUmMvDiz+1/bjSmNgMXwA4QYAUKl+bRqoQ9O6yszOs1dNBrwd4QYAcMmBxfdcvJDgu9/QNQXvR7gBAFzSD/u0UmhQoHYcS9W3DCyGlyPcAAAuqWHdUI28srm9/w6tN/ByhBsAQJXc3b+ga+rDrceUnpVLrcFrEW4AAFVyVftGatekrjKy8/QvBhbDixFuAADVGFgca++/R9cUvBjhBgBQrYHFIUEB9mrFO45xxWJ4J8INAKDKGtcL003dCwYWv7eBaeHwToQbAEC13HvxmjdLthxXZjYDi+F9CDcAgGq5qn1jtW0cYWdMzV8br+zcgpXDAW9BuAEAVO+LI/A/VyyetWKv+j/3qZ5c/K02HEpRfr6L2oTHBXu6AAAA3/OTa9rp3Pkc/WPTUSWmZemdrxPs1rJBHY3pHaMxvVqqU3Skp4sJP0W4AQBUW0hQoJ74wRV6bEQXrT+YrMVbjumTHSd17Ox5vbLygN2uaB6pXrEN1LFZPXWOjlSn6HpqHhVup5QDNcmnwk18fLx+8pOf6NSpUwoKCtL69etVt25dTxcLAPxWUGCAru7YxG6/GROnT3efsgONV+1J1Hcn0+xWXL2wYBt2OjWrp1YNIxQUWHD9HJN3AhSgQHMbIAVWIQBVFpLCggN1a68YRYWHuOV9wrf4VLh58MEH9Zvf/EZDhgxRSkqKwsLCPF0kAMBF4SFBuqVHjN3OZGRr7f4k7T+Vpn2J6XY7lJRhByFvPXLWbjVt94lUPXf7lfx+/JDPhJudO3cqJCTEBhujUaNGni4SAKCShTZv7RlTYp+ZVXUoOUP7Tpmwk6ZTqReUny+5zP9ckhmL/J/7lQ9MruxpMz39092J+nDrcf3vLd1s6IJ/cVu4WbNmjV566SVt2rRJJ06c0OLFizVmzJgSr5kzZ459jXm+e/fumj17dlFYuZR9+/apXr16uvXWW3X06FHdeeedevLJJ91VfABADQsNDrRjb8wmtaixn2NmbA15caUd/7N816kyIQvO57ap4BkZGerZs6defvnlcp9fuHChJk+erKeeekpbtmyxoWbkyJFKSPjPFS779u2ruLi4Mtvx48eVk5OjtWvX6pVXXtFXX32lFStW2A0AgBJfbIEBuqNPS3t/0aajVI4fclvLjQkqZqvIrFmzNH78eE2YMME+Nq02y5Yt09y5czVz5ky7z7T6VKRVq1bq37+/YmMLFm0bNWqUtm7dquHDh5f7+qysLLsVSk1NtbcmJJnNnQqP5+7jOh31Rp1xrnk3X/6M3tojWv/3+X6t3XdaR5PTFB0VXms/25frzVMqq7PLqcdaGXOTnZ1tg8u0adNK7B8xYoTWrVtXpWOYYGNmSZ05c0b169e33WA//elPK3y9CUwzZswos3/58uWKiIhQTaAliXqrLZxr1Ftt8tXzrW29IB1KD9BLf1+p62Nq/+KCvlpvnlRenWVmZnpnuElKSlJeXp6io6NL7DePT548WaVjBAcH6/nnn9e1114rl8tlg9Ett9xS4eunT5+uqVOnlmi5Ma0+5t9FRUXJnUyqNL8Q04pkBj2DeqspnGvUW23y9fPtXNMj+vWHu7X7fH29NHJQrV1fx9frzRMqq7PCnhevnS1V+sQyIaU6J9ulur6KM9PEy5sqbiqtpk62mjy2k1Fv1Bnnmnfz1c/obb1j9Zule7Q3MV17T59XXMv6tfrzfbXePKm8OrucOqyVtaWaNGliL7pXupUmMTGxTGsOAADuUL9OiIZ3K/iOMctEwH/USrgJDQ21M6FK96WZx4MHD66NIgAA/NCdfVrZ2w+3HWf1cj/itm6p9PR07d+/v8RSCWY2k7nYXuvWre34l/vvv1/9+vXToEGDNG/ePDsNfOLEie4qAgAAJQzp1ERNI8N0Oi3LLgkxontzasgPuC3cbNy4UcOGDSt6XDiYd9y4cVqwYIHGjh2r5ORkPfvss/Yifub6NUuXLlWbNm3cVQQAAEoIDgrUmF4x+svaeH2w+Rjhxk+4LdwMHTrUDhCuzKRJk+wGAEBt+WHfVjbcfPbdKbvmlVkaAs5WK2NuAADwlCuaR6lbiyjl5Ln0r+3H+UX4AcINAMAvWm8MlmPwD4QbAIDj3dYrRsGBAdp29Jz2J6Z5ujioYYQbAIDjNakXpqFdmtr7izYf83RxUMMINwAAv/DDi9e8Wbz5mPLya3+tKdQewg0AwC9c37WZvWrxydQLWncgydPFQQ0i3AAA/EJYcJBu7Rlj7zOw2NkINwAAv5s19cnOkzp6JtPTxUENIdwAAPxGz1b11aFpXV3IydeQF1fqrle/0tvrDyslI9vTRYMbEW4AAH4jICBAf7y7twa0bSRzUf1v4lP0qyU7NOC5T/XgG9/og81HlZ6V6+liwluWXwAAwBfEtayvv08cpONnz+uj7cftiuE7jqVq1Z7TdgsL/laDOzRW5+hIdWhWTx2a1lPHZvXsYGT4BsINAMAvxTSoo/++toPdDpxO17+2FQSdg6cztHLPabsVZ1YX79i0njo0q6uGEaEKMDsDAhQYIAWY/wXI3s/Pz9feYwFKWH1QgUFB5f5s89qKtGlUVzf3aOHmd+tfCDcAAL9nWmcm39hZj97QSTuPp2pLwhntT0zXgdMZ9tZMHz+dlmW3rw4mV6G+gvRRwv7LrtfYRlerR6sGfv97uVyEGwAAio3JMd1WZisu7UKODToHbOBJV0ZWrsxlAPNdLjt2x9x3Xbyfm5evI0eOKDY2VoGmKacU85qKbDiUokPJmfr6YArh5nsg3AAAcAmR4SHqFdvAbpeSk5OjpUsPa9So7goJqd44nT+vPqAX/v2dNh0+o//it3LZmC0FAICX6Numob3dlHDGtgTh8hBuAADwEle2rK+QoAA7tufomfOeLo7PItwAAOAlwkOC1C2mYLzP5oQzni6OzyLcAADgRfq2vtg1dZhwc7kINwAAeOO4G8LNZSPcAADgRfq0KZiR9d3JNDvlHNVHuAEAwIu0qF9HMfXDlZfv0rajZz1dHJ9EuAEAwMv0udg1tZmuqctCuAEAwMv0uTioeHMCLTeXg3ADAICXDio208Hz87mYX3URbgAA8DLdYqIUHhKos5k5OpiU4eni+BzCDQAAXiYkKFA9WhbMmuJiftVHuAEAwAsxqPjyEW4AAPBCXMzv8hFuAADwQr1bF3RL7UtM17nzOZ4ujk8h3AAA4IWa1AtT28YR9v4WFtGsFsINAABeinE3l4dwAwCAl+JifpeHcAMAgJcPKjbdUmatKVQN4QYAAC/VOTpS9cKClZGdpz0n0zxdHJ9BuAEAwEsFBQaoV2zBrKlNDCquMsINAAA+MKh4CyuEOzPc/OEPf1D37t3VrVs3PfLII3K56H8EAPjJxfxouXFeuDl9+rRefvllbdq0Sd9++629Xb9+vaeLBQBAjTLdUgEB0uHkTCWlZ1HbTgo3Rm5uri5cuKCcnBy7NWvWzNNFAgCgRtWvE6JOzerZ+5vpmqrdcLNmzRqNHj1aMTExCggI0JIlS8q8Zs6cOWrXrp3Cw8PVt29frV27tsrHb9q0qR5//HG1bt3a/owbb7xRHTp0cFfxAQDwWnRNeSjcZGRkqGfPnrbrqDwLFy7U5MmT9dRTT2nLli0aMmSIRo4cqYSEhKLXmMATFxdXZjt+/LjOnDmjjz76SIcOHdKxY8e0bt06G6gAAPCbi/nRclMlwXITE1TMVpFZs2Zp/PjxmjBhgn08e/ZsLVu2THPnztXMmTPtPjOOpiLvv/++OnbsqEaNGtnHN998sx1zc+2115b7+qysLLsVSk1NtbeFXVruVHg8dx/X6ag36oxzzbvxGfWeeusRE2lvtx89p4zzWQoN9qlRJd+rzi6nHt0WbiqTnZ1tg8u0adNK7B8xYoRtgamK2NhY+1oz5iYkJESrVq3Sf//3f1f4ehOYZsyYUWb/8uXLFRFRsBCZu61YsaJGjut01Bt1xrnm3fiMer7ezOTgiOAgZebm67VFn6hNQdbxizrLzMz0znCTlJSkvLw8RUdHl9hvHp88ebJKx7jqqqs0atQo9e7dW4GBgbrhhht06623Vvj66dOna+rUqSVabkxAMoEqKipK7mRSpfmFDB8+3AYvUG81hXONeqtNnG/eVW//PLNZK/ckKTy2u0YNbiN/qbPUiz0vXhduCpmBxsWZ69SU3leZ5557zm5VERYWZrfSTKXVVACpyWM7GfVGnXGueTc+o95Rb/3aNrbhZtuxVMd+14SUU2eX815rpdOuSZMmCgoKKtNKk5iYWKY1BwAAVDyoeOOhFBbR9IZwExoaamdCle5LM48HDx5cG0UAAMDnL+YXFR6sU6lZWrTpqKeL4x/hJj09XVu3brWbER8fb+8XTvU241/mz5+v119/Xbt379aUKVPscxMnTnRXEQAAcKw6oUF6+PpO9v7vlu9RRlaup4vktdw25mbjxo0aNmxY0ePCwbzjxo3TggULNHbsWCUnJ+vZZ5/ViRMn7PVrli5dqjZtnDUoCgCAmvLA4DZ6a/1hJaRkat6ag5oyvDOVXZPhZujQoZdcyHLSpEl2AwAA1RcWHKRpI6/QpL9t1qtrDuieAa3VvH44VVmKs64CBACAw42Ma65+bRrqQk6+7Z5CWYQbAAB8iLmEylM3d7X3F20+qh3Hznm6SF6HcAMAgI/p3bqhbu0ZY69c/NzHuy85LMTfEG4AAPBBT/ygi11j6quDyfpsd6Kni+NVCDcAAPigVg0jNP6advb+80t3Kycv39NF8hqEGwAAfNSkoR3UuG6oDiZl6J2vC64rB8INAAA+KzI8pOhaN7M/3atz53M8XSSvQMsNAAA+7O7+serUrJ7OZObolZX7PV0cr0C4AQDAhwUHBerJi1PDF3x5SAnJmfJ3hBsAAHzc0M5NNaRTE2Xn5Wv2Z3vl7wg3AAA44MJ+PxvW0d5fuy/J7697Q7gBAMABerZqoKDAAJ1Oy9LJ1AvyZ4QbAAAcoE5okDpHR9r7247495IMhBsAAByiZ6v69nb70bPyZ4QbAAAcomdsA3u7jXADAACcoEdRy8055ef772KatNwAAOAQZsxNWHCg0i7k6lByhvwV4QYAAIcICQpUXMuC1ht/7poi3AAA4MCuqW1+PGOKcAMAgMOud+PvM6YINwAAOLDlZufxVOXk5csfEW4AAHCQto3rKio8WFm5+dpzMk3+iHADAICDBAYGqEdR15R/jrsh3AAA4NhBxWfljwg3AAA4TE8/v1Ix4QYAAIfOmNqXmK7M7Fz5G8INAAAO07x+uJpFhikv32VnTfkbwg0AAA7U42LrjT+OuyHcAADgQL1i/7OIpr8h3AAA4OSWm6O03AAAAAdNBz+cnKmzmdnyJ7TcAADgQA0iQtW2cYRfdk0RbgAAcKgefjqomHADAIDTr1R8lJYbAADgsCsVu1wu+QtabgAAcKjuMVEKCgzQ6bQsnUy9IH/hleHm9ttvV8OGDXXnnXeWee6jjz5Sly5d1KlTJ82fP98j5QMAwBdEhAarU7N69v62I/7TNeWV4eaRRx7Rm2++WWZ/bm6upk6dqs8//1ybN2/Wb3/7W6WkpHikjAAA+NI6U9v96Ho3Xhluhg0bpsjIyDL7v/nmG3Xv3l0tW7a0z48aNUrLli3zSBkBAPAFPf1whfBqh5s1a9Zo9OjRiomJUUBAgJYsWVLmNXPmzFG7du0UHh6uvn37au3atW4p7PHjx22wKdSqVSsdO3bMLccGAMDJM6a2Hz2n/Hz/GFRc7XCTkZGhnj176uWXXy73+YULF2ry5Ml66qmntGXLFg0ZMkQjR45UQkJC0WtM4ImLiyuzmfBSmfJGepuABQAAyteleaTCggOVdiFXh5Iz/KKagqv7D0xQMVtFZs2apfHjx2vChAn28ezZs23X0dy5czVz5ky7b9OmTZdVWNNqU7yl5ujRoxo4cGC5r83KyrJbodTUgiXfc3Jy7OZOhcdz93GdjnqjzjjXvBufUefUW7cWkdpy5Jw2H0pWbIMw+VKdXU49VjvcVCY7O9sGl2nTppXYP2LECK1bt+57H3/AgAHasWOHDThRUVFaunSpfv3rX5f7WhOkZsyYUWb/8uXLFRFRcDlqd1uxYkWNHNfpqDfqjHPNu/EZ9f16i8wxHTWB+vDL7Qo5vlW+VGeZmZmeDTdJSUnKy8tTdHR0if3m8cmTJ6t8nJtuusnOhjJdYGZczeLFi9W/f38FBwfr97//vR1wnJ+fryeeeEKNGzcu9xjTp0+3M6uKt9zExsbaoGWCkTuZVGl+IcOHD1dISIhbj+1k1Bt1xrnm3fiMOqfecrYe15pFO5QW0lCjRpXf4+GtdVbY8+KxcFPROBgzVqY6Y2MqmwF166232u1SwsLC7FaaqbSaOtlq8thORr1RZ5xr3o3PqO/XW++2BQ0Bu06kSYFBCgkK9Jk6u5w6dOu7a9KkiYKCgsq00iQmJpZpzQEAALWjXeO6igwPVlZuvvacTHN8tbs13ISGhtqZUKX7zMzjwYMHu/NHAQCAKgoMDCgxJdzpqh1u0tPTtXXrVrsZ8fHx9n7hVG8zzsUsi/D6669r9+7dmjJlin1u4sSJ7i89AACokl4XL+b3dXyy42us2mNuNm7caAf0FioctDtu3DgtWLBAY8eOVXJysp599lmdOHHCXr/GzGpq06aNe0sOAACqbEinpnpl5QGt2Xtaefkuu6CmU1U73AwdOvSSy6ZPmjTJbgAAwDv0bdNQkWHBOpOZY9eZ6t26oZzKO4dLAwAAtwoJCtTVHZvY+6v2nHZ07RJuAADwE0O7NLW3q/YSbgAAgANcdzHcmG6p5PT/LFHkNLTcAADgJ1rUr6MrmkfKDJ1duy9JTkW4AQDAjwzt0szertqTKKci3AAA4IfjbtbsS1J+fuWzn30V4QYAAD+cEp6Ska1vjznzasWEGwAA/EiIH0wJJ9wAAOC3U8IT5USEGwAA/HRK+NYjZ3UmI1tOQ7gBAMCPp4Sv2ee8rinCDQAAftx6s9qB424INwAA+KGhnQuud7N672nHTQkn3AAA4If6tW2oemHBSs7I1o7jzpoSTrgBAMBvp4Q3duSUcMINAAB+aqhDl2Ig3AAA4OfXu9l65KzOZjpnSjjhBgAAP54S3iU6UmY8sVlryikINwAA+LGhhVcrdlDXFOEGAAA/dl3hKuEOmhJOuAEAwI/1a9NIdUODlJSerZ3HU+UEhBsAAPxYaHDxVcKd0TVFuAEAwM8NLZwSvtcZ17sh3AAA4OeGXhx3syXhjCOmhBNuAADwczEN6qhTs3p2Svg38SnydYQbAACg7jFRthYOnM7w+dog3AAAALVvWs/WwsHT6T5fG4QbAACg9k3r2lo4mETLDQAAcIB2TS6GG1puAACAk8LNmcwcncnw7RlTdEsBAABFhAYrpn64I7qmCDcAAMBRg4oJNwAAoOS4G1puAACAo2ZMnablBgAAOKhbKp6WGwAA4ATtL3ZLHUrOVJ5Zi8FHMeYGAAAUrTEVGhyo7Nx8HTtzXr7KK8PN7bffroYNG+rOO+8ssf/IkSMaOnSounXrph49euj999/3WBkBAHCaoMAAtWtc0HpzIMl3x914Zbh55JFH9Oabb5bZHxwcrNmzZ2vXrl369NNPNWXKFGVk+PZcfAAAvHFQcbwPL6DpleFm2LBhioyMLLO/RYsW6tWrl73frFkzNWrUSCkpvr80OwAA3jcdPF1+E27WrFmj0aNHKyYmRgEBAVqyZEmZ18yZM0ft2rVTeHi4+vbtq7Vr18rdNm7cqPz8fMXGxrr92AAA+Kv2RRfy86OWG9MN1LNnT7388svlPr9w4UJNnjxZTz31lLZs2aIhQ4Zo5MiRSkhIKHqNCTxxcXFltuPHj1epDMnJyXrggQc0b9686hYfAABU6Vo3vhtugqv7D0xQMVtFZs2apfHjx2vChAn2sRkjs2zZMs2dO1czZ860+zZt2nTZBc7KyrIDjqdPn67BgwdX+jqzFUpNTbW3OTk5dnOnwuO5+7hOR71RZ5xr3o3PqH/WW2z9MHt7MvWCzqafV92wakcFt9bZ5dSjW0ucnZ1tg8u0adNK7B8xYoTWrVv3vY/vcrn04IMP6vrrr9f9999f6WtNkJoxY0aZ/cuXL1dERIRqwooVK2rkuE5HvVFnnGvejc+o/9Vb3eAgZeQG6G8fLlergoYcj9VZZmamZ8NNUlKS8vLyFB0dXWK/eXzy5MkqH+emm27S5s2bbRdYq1attHjxYvXv319ffvml7fYy08ALx/q89dZbuvLKK8scw7TsTJ06tUTLjRmfY4JWVFSU3MmkSvMLGT58uEJCQtx6bCej3qgzzjXvxmfUf+vtzWPfaFPCWcV06a1RPVp4tM4Ke16qo0bamsxA49ItLqX3VcZ0Y5XnmmuusYOIqyIsLMxupZlKq6mTrSaP7WTUG3XGuebd+Iz6X711aFbPhpvDZy7U6nsor84u5+e7dSp4kyZNFBQUVKaVJjExsUxrDgAA8E7tfXyNKbeGm9DQUDsTqnSfmXlc2eBfAADghde6Oe2b4aba3VLp6enav39/0eP4+Hht3brVXlCvdevWdpyLGezbr18/DRo0yE7XNtPAJ06c6O6yAwCAGtChaDp4erWHlvhkuDEXzzNXEC5UOGh33LhxWrBggcaOHWuvQ/Pss8/qxIkT9vo1S5cuVZs2bdxbcgAAUCNaN6qrwAApIztPiWlZio4Kd3a4MQtXmhRXmUmTJtkNAAD4ntDgQMU2itDh5EwdOJ3uc+HGK9eWAgAAntX+4rgbXxxUTLgBAACOWmOKcAMAACpZY8r3Vgcn3AAAgIqng9MtBQAAnKDDxW6pIymZys6t2uoA3oKWGwAAUEazyDDVDQ1SvktKSPGtcTeEGwAAUIa5cF/hoOIDPjaomHADAAActQwD4QYAAFQ6Yyo+ybdmTBFuAACAo651Q7gBAACVXqXY16aDE24AAEClY25SMrJ1NjNbvoJwAwAAylU3LFjNLy6a6UutN4QbAABQhWUYCDcAAMBR08HT5StouQEAAI6aMUW4AQAAVbjWDeEGAAA4QIcmBS038ckZyjMLTfkAWm4AAECFWjaso9CgQLsy+PGz5+ULCDcAAKBCQYEBatM4wt4/4CODigk3AADAUdPBCTcAAKBKM6Z8ZVAx4QYAAFTtWjc+sjo44QYAAFSqA91SAADASdpfnA5+4twFZWbnytvRcgMAACrVsG5o0f2MrDx5O8INAABwFMINAABwFMINAABwFMINAABwFMINAABwFMINAABwFMINAABwFMINAABwFMINAAC4pIAA+QzCDQAAcBTCDQAAcBTCDQAAcBTCDQAAcBTCDQAAcBTCDQAAcJRg+QmXy2VvU1NT3X7snJwcZWZm2mOHhIS4/fhORb1RZ5xr3o3PKPVWXH5WpsxXaWrqOYW5wlVb51rh93bh93hVBLiq82ofdvToUcXGxnq6GAAA4DIcOXJErVq1qtJr/Sbc5Ofn6/jx44qMjFSAm69EZFKlCU6m4qOiotx6bCej3qgzzjXvxmeUevOGc83ElLS0NMXExCgwsGqjafymW8pUSFUT3+UyvxDCDfVWGzjXqLfaxPlGvXn6XKtfv361jsOAYgAA4CiEGwAA4CiEGzcICwvT008/bW9BvdUkzjXqrTZxvlFvvnqu+c2AYgAA4B9ouQEAAI5CuAEAAI5CuAEAAI5CuAEAAI5CuPme5syZo3bt2ik8PFx9+/bV2rVr3fObcYg1a9Zo9OjR9sqS5srQS5YsKfG8Gc/+zDPP2Ofr1KmjoUOHaufOnfJ3M2fOVP/+/e0VtZs1a6YxY8Zoz549JV5D3ZU0d+5c9ejRo+giYIMGDdK///1v6usyzj3zWZ08eTJ1Vwnzd8vUU/GtefPm1NklHDt2TD/+8Y/VuHFjRUREqFevXtq0aZPb/64Rbr6HhQsX2j8ATz31lLZs2aIhQ4Zo5MiRSkhI+D6HdZSMjAz17NlTL7/8crnPv/jii5o1a5Z9fsOGDfaPw/Dhw+2ltv3Z6tWr9bOf/Uzr16/XihUrlJubqxEjRtj6LETdlWSuQP7CCy9o48aNdrv++ut12223Ff1hpL4uzXwG582bZ0NicdRd+bp3764TJ04Ubd9++y11VokzZ87o6quvtgtjmv/w2LVrl37/+9+rQYMG7j/XzFRwXJ4BAwa4Jk6cWGLfFVdc4Zo2bRpVWg5zui1evLjocX5+vqt58+auF154oWjfhQsXXPXr13f9+c9/pg6LSUxMtPW3evVq6q4aGjZs6Jo/fz7nWhWkpaW5OnXq5FqxYoXruuuucz366KOca5V4+umnXT179iz3Of62le+Xv/yl65prrqngWffWGy03lyk7O9s2pZn/mi7OPF63bt3lHtavxMfH6+TJkyXq0FzA6brrrqMOSzl37py9bdSoEXVXBXl5eXrvvfdsS5fpnuJcuzTTUnjzzTfrxhtv5HNaRfv27bPdJ2Zowt13362DBw/y+azEhx9+qH79+ulHP/qR7W7v3bu3/vKXvxQ9787PKeHmMiUlJdk/oNHR0SX2m8fml4NLK6wn6rByptFr6tSpuuaaaxQXF0fdVcJ0C9SrV8/+QZw4caIWL16sbt26ca5dggmCmzdvtuNt+JxWzcCBA/Xmm29q2bJl9gva/D0bPHiwkpOTOd8qYMKfGRvXqVMnW2/mM/rII4/YenT3d4LfrApeU8wgstJfRKX3gTr8Pn7+859r+/bt+uKLLzj/LqFLly7aunWrzp49q0WLFmncuHF2/BKf14odOXJEjz76qJYvX24nRlSEv3UlmfGVha688krbQtihQwf99a9/1VVXXUWdlSM/P9+23Dz//PP2sWm5MWPiTOB54IEH3Hqu0XJzmZo0aaKgoKAyaTIxMbFM6kT5CmcWUIcVe/jhh21T7sqVK+2AWequcqGhoerYsaP9A2paIcxg9j/+8Y+ca5Uw3evm75aZ7RkcHGw3Ewj/9Kc/2fuFf8/4nFaubt26NuSYrir+tpWvRYsWtiW1uK5duxZNwnFnvRFuvscfUfPHwMxkKc48Nk2TuDTTT21O5uJ1aMYymT+s/l6H5r9UTIvNBx98oM8//9zWVXHUXdXrMSsri/qqxA033GC780yLV+FmwuF9991n77dv357PaRWY82z37t32C5zPZ/nMTKnSl7TYu3ev2rRpY++7td6qNfwYJbz33nuukJAQ12uvvebatWuXa/Lkya66deu6Dh06RE0Vm4GxZcsWu5nTbdasWfb+4cOH7fNmVLwZCf/BBx+4vv32W9c999zjatGihSs1NdWv6/B//ud/bL2sWrXKdeLEiaItMzOz6DXUXUnTp093rVmzxhUfH+/avn2768knn3QFBga6li9fTn1VU/HZUpxr5Xvsscfs5/PgwYOu9evXu2655RZXZGRk0d9/Pp9lffPNN67g4GDXc88959q3b5/rb3/7mysiIsL19ttvF73GXfVGuPmeXnnlFVebNm1coaGhrj59+hRN1UWBlStX2lBTehs3blzR1D8zpdJM/wsLC3Nde+219oT2d+XVmdneeOONotdQdyX95Cc/KfosNm3a1HXDDTcUBRvq6/uFG861ssaOHWu/dM1/4MbExLjuuOMO186dO6mzS/jXv/7liouLs3/vzaVT5s2bV+J5d51rAeb/qtfWAwAA4L0YcwMAAByFcAMAAByFcAMAAByFcAMAAByFcAMAAByFcAMAAByFcAMAAByFcAMAAByFcAPAK5nF88zCoWZto7CwMMXGxmr06NH67LPPvvexFyxYoAYNGrilnAC8T7CnCwAApR06dMgusmcCyIsvvqgePXooJydHy5Yt089+9jN99913VBqACtFyA8DrTJo0SQEBAfrmm2905513qnPnzurevbumTp2q9evX29ckJCTotttuU7169RQVFaW77rpLp06dKjrGtm3bNGzYMEVGRtrn+/btq40bN2rVqlV66KGHdO7cOfszzPbMM8948N0CcDfCDQCvkpKSok8++cS20NStW7fM86Y1xyyJN2bMGPva1atXa8WKFTpw4IDGjh1b9Lr77rtPrVq10oYNG7Rp0yZNmzZNISEhGjx4sGbPnm0Dz4kTJ+z2+OOP1/K7BFCT6JYC4FX2799vw8sVV1xR4Ws+/fRTbd++XfHx8XYsjvHWW2/Z1h0TZvr3729bdn7xi18UHadTp05F/75+/fq2xaZ58+a18I4A1DZabgB4FRNsDBM+KrJ7924bagqDjdGtWzfbqmOeM0wX1oQJE3TjjTfqhRdesC07APwD4QaAVzEtLCbYFIaUigJQeeGn+H4zjmbnzp26+eab9fnnn9vws3jx4hotOwDvQLgB4FUaNWqkm266Sa+88ooyMjLKPH/27FkbVEy305EjR4r279q1yw4S7tq1a9E+MxB5ypQpWr58ue644w698cYbdn9oaKjy8vJq6R0BqG2EGwBeZ86cOTZ8DBgwQIsWLdK+fftsS86f/vQnDRo0yHY1menhZtDw5s2b7ayqBx54QNddd5369eun8+fP6+c//7mdGXX48GF9+eWXdixOYfBp27at0tPT7TVzkpKSlJmZ6em3DMCNCDcAvE67du1saDFTuR977DHFxcVp+PDhNozMnTvXdj0tWbJEDRs21LXXXmvDjrnY38KFC+2/DwoKUnJysg08pvXGTBMfOXKkZsyYYZ83M6YmTpxoZ1c1bdrUXksHgHMEuApH7wEAADgALTcAAMBRCDcAAMBRCDcAAMBRCDcAAMBRCDcAAMBRCDcAAMBRCDcAAMBRCDcAAMBRCDcAAMBRCDcAAMBRCDcAAMBRCDcAAEBO8v8BgCadVNtIDeQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_t=non_lin_cst(func.Rosen(),square2(),10)\n",
    "costs,x=central_path(f_t,10,1.e-9,np.array([0,0]))\n",
    "cost_min=np.min(costs)\n",
    "\n",
    "plt.plot(costs - cost_min)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Cost')\n",
    "plt.title('Evolution of the cost')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> METTEZ ICI VOTRE ARGUMENT POUR JUSTIFIER DE LA CONVERGENCE LINEAIRE DE CET ALGORITHME\n",
    "\n",
    "\n",
    "La convergence est linéaire car on a :\n",
    "\n",
    "$f(x_k) - f(x^\\star) \\leq \\frac{p}{t_k}$,\n",
    "\n",
    "donc $ \\frac{||f(x_{k+1}) - f(x^\\star) ||}{||f(x_{k}) - f(x^\\star) ||} \\leq \\frac{p \\cdot t_{k+1}}{t_k \\cdot p} = \\frac{\\mu^{k+1} \\cdot t_0}{\\mu^k \\cdot t_0} = \\frac{1}{\\mu} \\in ]0,1[ $\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant s'intéresser à l'influence de $\\mu$. On fixe maintenant `varepsilon` à $10^{-6}$ et on va tracer la courbe d'évolution de la fonction à minimiser (en échelle log, et en traçant la fonction auquelle on a retiré `costmin`) pour différentes valeurs de $\\mu$ dans `mu_list` ci-dessous. La valeur de `cost_min` est inchangée et est la valeur trouvée pour `varepsilon` à $10^{-9}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'non_lin_cst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m mu_list=[\u001b[32m2\u001b[39m,\u001b[32m4\u001b[39m,\u001b[32m7\u001b[39m,\u001b[32m10\u001b[39m,\u001b[32m100\u001b[39m,\u001b[32m1000\u001b[39m]\n\u001b[32m      2\u001b[39m varepsilon = \u001b[32m1.e-6\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m f_t=\u001b[43mnon_lin_cst\u001b[49m(func.Rosen(),square2(),\u001b[32m10\u001b[39m)\n\u001b[32m      4\u001b[39m costs,x=central_path(f_t,\u001b[32m10\u001b[39m,\u001b[32m1.e-9\u001b[39m,np.array([\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m]))\n\u001b[32m      5\u001b[39m cost_min=np.min(costs)\n",
      "\u001b[31mNameError\u001b[39m: name 'non_lin_cst' is not defined"
     ]
    }
   ],
   "source": [
    "mu_list=[2,4,7,10,100,1000]\n",
    "varepsilon = 1.e-6\n",
    "f_t=non_lin_cst(func.Rosen(),square2(),10)\n",
    "costs,x=central_path(f_t,10,1.e-9,np.array([0,0]))\n",
    "cost_min=np.min(costs)\n",
    "costs_list = []\n",
    "\n",
    "for mu in mu_list:\n",
    "    f_t=non_lin_cst(func.Rosen(),square2(),mu)\n",
    "    costs,x=central_path(f_t,10,varepsilon,np.array([0,0]))\n",
    "    costs_list.append(costs - cost_min)\n",
    "    \n",
    "plt.figure(figsize=(7,4))\n",
    "for i, costs_to_plot in enumerate(costs_list):\n",
    "    plt.plot(costs_to_plot, label=f'mu = {mu_list[i]}')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Cost - cost_min')\n",
    "plt.xlabel('iterations')\n",
    "plt.title('Evolution of the cost')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyez-vous une grande différence quand $\\mu$ varie ?\n",
    "<div class=\"alert alert-block alert-info\"> METTEZ ICI VOTRE ARGUMENT \n",
    "\n",
    "\n",
    "On n'observe pas de différences réellement significatives quand $\\mu$ varie. Lorsqu'on prend $\\mu$ petit, alors la barrière se réduit lentement et l'ensemble admissible s'aggrandit donc lui aussi lentement : l'algorithme de Newton converge alors en peu d'itérations vers le minimiseur global de chaque $f_{t_k}$ (\"marche d'escaliers\" petites) mais il faut alors lancer Newton plus de fois pour atteindre l'extension complète de l'ensemble admissible et converger vers le minimiseur global de $f$. Lorsque $\\mu$ est grand, le phénomène inverse a lieu : l'algorithme de Newton a besoin de plus d'itérations pour converger vers les minimseurs globaux de $f_{t_k}$ puisque la frontière se réduit fortement à chaque lancement mais on nécessite alors moins de lancements pour atteindre le minimiseur global de $f$. Ce phénomène de compensation entre le nombre d'itérations intra-Newton et le nombre de lancement de Newton conduit ainsi à une vitesse de convergence similaire quelque soit la valeur de $\\mu$. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problème avec multiples contraintes linéaires\n",
    "On s'intéresse maintenant aux problèmes de la forme\n",
    "$$\n",
    "\t(\\mathcal{P}_{t, \\text{ lin.}}) \\qquad \\umin{x\\in \\mathbb{R}^d, } f_t(x) \\eqdef f(x) - \\frac{1}{t} \\text{Log}( b-A x  ),\n",
    "$$\n",
    "qui approximent, quand $t$ tend vers $+\\infty$ des problèmes du genre\n",
    "$$\n",
    "    (\\mathcal{P}_{\\infty,\\text{ lin.}}) \\qquad \\umin{x\\in \\mathbb{R}^n, A x \\leq b} f(x)\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> Calculez ci-dessous le gradient et la Hessienne de $f_t$ en fonction de $A$,$b$,t et du gradient et de la Hessienne de $f$  \n",
    "$$\\nabla f_t(x) = ?? \\quad H[f_t](x) = ?? $$\n",
    "\n",
    "$$\n",
    "f_t(x) = f(x) + \\sum_{i=1}^{n} \\ln\\bigl(b_i - (Ax)_i\\bigr)\n",
    "$$\n",
    "\n",
    "Soit $k,l \\in \\{1,..,n\\}$\n",
    "\n",
    "Gradient :\n",
    "$$\n",
    "\\partial_k f_t(x) \n",
    "= \\partial_k f(x) \n",
    "- \\sum_{i=1}^{n} \n",
    "\\frac{\\partial_k \\bigl( \\sum_{j=1}^{n} A_{ij} x_j \\bigr)}{b_i - (Ax)_i}\n",
    "= \\partial_k f(x) \n",
    "- \\sum_{i=1}^{n} \n",
    "\\frac{A_{ik}}{b_i - (Ax)_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla f_t(x) = \\nabla f(x) - A^\\top v,\n",
    "\\quad \\text{avec } v_i = \\frac{1}{b_i - (Ax)_i}.\n",
    "$$\n",
    "\n",
    "Hessienne :\n",
    "\n",
    "$$\n",
    "\\partial_k \\partial_l f_t(x) \n",
    "= \\partial_k \\partial_l f(x) \n",
    "- \\sum_{i=1}^{n} \\partial_l \\left( \\frac{A_{ik}}{b_i - (Ax)_i} \\right)\n",
    "= \\partial_k \\partial_l f(x) \n",
    "+ \\sum_{i=1}^{n} \\frac{A_{ik} A_{il}}{(b_i - (Ax)_i)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla^2 f_t(x) = \\nabla^2 f(x) + A^\\top D A,\n",
    "\\quad \\text{avec } D = \\mathrm{diag}\\left( \\frac{1}{(b_1 - (Ax)_1)^2}, \\dots, \\frac{1}{(b_n - (Ax)_n)^2} \\right)_{1 \\leq i \\leq n}\n",
    "$$\n",
    "</div>\n",
    "Créez une classe de fonction `class lin_cst()` qui prend au constructeur la fonction `f`, la matrice `A`, le vecteur `b` le scalaire `t` et  et qui calcule la valeur de $f_t$. Cette fonction devra agir comme `class non_lin_cst()` et notamment avoir un attribut `nb_constraints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lin_cst() :\n",
    "    def __init__(self,f,A,b,t) :\n",
    "        self.zeros()\n",
    "        self.f=f\n",
    "        self.A=A\n",
    "        self.b=b\n",
    "        self.t=t\n",
    "        self.n = b.shape[0]\n",
    "        self.nb_constraints=1.\n",
    "    def zeros(self) :\n",
    "        self.nb_eval=0\n",
    "        self.nb_grad=0\n",
    "        self.nb_hess=0\n",
    "    def value(self,x) :\n",
    "        self.nb_eval += 1\n",
    "        g=A@x - b\n",
    "        l=0*g\n",
    "        l[g>=0]=np.inf\n",
    "        l[g<0]=-np.log(-g[g<0])\n",
    "        return self.f.value(x) + l.sum()/self.t    \n",
    "    def grad(self,x) :\n",
    "        self.nb_grad += 1\n",
    "        tmp=1./self.t*self.A.T@(1./(self.b-self.A@x))\n",
    "        return self.f.grad(x) + tmp    \n",
    "    def Hess(self, x):\n",
    "        self.nb_hess += 1\n",
    "        w = 1. / (self.b - self.A @ x)**2\n",
    "        D = np.diag(w)\n",
    "        tmp = (1./self.t) * (self.A.T @ D @ self.A)\n",
    "        return self.f.Hess(x) + tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va tester notre fonction dans la case ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.717787999246487 inf\n",
      "[-9.36529883 -1.43185282] [-624.84901174 -313.83869195]\n",
      "[[1624.61765301  896.38712898]\n",
      " [ 896.38712898  871.39523753]]\n",
      "[[2531.87148488  934.54404056]\n",
      " [ 934.54404056  476.99362034]]\n",
      "## TEST DE DERIVEE NUMERIQUE##\n",
      "eps 1.0e-01 grad inf ratio 8.1e-01 angle 2.0e+00\n",
      "eps 1.0e-02 grad 8.3e-01 ratio 1.9e+00 angle 4.8e-04\n",
      "eps 1.0e-03 grad 4.9e-02 ratio 6.8e-02 angle 4.7e-06\n",
      "eps 1.0e-04 grad 4.7e-03 ratio 6.4e-03 angle 4.7e-08\n",
      "eps 1.0e-05 grad 4.7e-04 ratio 6.3e-04 angle 4.7e-10\n",
      "eps 1.0e-06 grad 4.7e-05 ratio 6.3e-05 angle 4.7e-12\n",
      "eps 1.0e-07 grad 4.7e-06 ratio 6.3e-06 angle 4.7e-14\n",
      "eps 1.0e-08 grad 4.8e-07 ratio 6.3e-07 angle 4.4e-16\n",
      "eps 1.0e-09 grad 6.0e-08 ratio 6.5e-08 angle 2.2e-16\n",
      "eps 1.0e-10 grad 8.4e-07 ratio 3.6e-08 angle 0.0e+00\n",
      "eps 1.0e-11 grad 1.5e-05 ratio 2.8e-07 angle 1.0e-13\n",
      "eps 1.0e-12 grad 7.7e-05 ratio 8.7e-06 angle 8.9e-16\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "x=np.zeros(2)\n",
    "n=10\n",
    "A=np.random.randn(42,2)\n",
    "b=np.abs(np.random.randn(42))\n",
    "x2=np.random.randn(2)\n",
    "f_t=lin_cst(func.Rosen(),A,b,10)\n",
    "print(f_t.value(x),f_t.value(x2)) ## 4.717787999246487 inf\n",
    "print(f_t.grad(x),f_t.grad(x2)) ## [-9.36529883 -1.43185282] [-624.84901174 -313.83869195]\n",
    "print(f_t.Hess(x)) ## [[1624.61765301  896.38712898] [ 896.38712898  871.39523753]]\n",
    "print(f_t.Hess(x2)) ## [[2531.87148488  934.54404056][ 934.54404056  476.99362034]]\n",
    "\n",
    "print('## TEST DE DERIVEE NUMERIQUE##')\n",
    "d=np.random.randn(2)\n",
    "opt.deriv_num(f_t,5.e-3*x2,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'intéresse maintenant au problème particulier du [Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics)), pour retrouver un signal parcimonieux. Le problème du Lasso s'écrit $$\n",
    "        \\umin{w \\in \\mathbb{R}^p} \\frac{1}{2}\\Vert Bw-y \\Vert_2^2 + \\lambda \\Vert x \\Vert_1\n",
    "$$\n",
    "On suppose ici qu'on veut retrouver un signal $w_0$ que l'on sait être parcimonieux (beaucoup de coefficients nuls), quand on n'observe que $y=Bw_0+N$ où $B$ est l'opérateur d'observation dans $ \\mathbb{R}^{n \\times p}$ avec $n<<p$ et $N$ est un bruit.  On va supposer ici que $B$ est une matrice aléatoire Gaussienne, ce qui pose notre problème dans le cadre du [compressed sensing](https://en.wikipedia.org/wiki/Compressed_sensing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40\n",
    "p = 60\n",
    "np.random.seed(42)\n",
    "B = np.random.randn(n,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée `w0` le vecteur que l'on souhaite retrouver, qui est parcimonieux et on génère le signal $y=Bw_0+N$ où $N$ est un bruit gaussien additif. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22 57 43 35]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "w0 = np.zeros(p) \n",
    "I=(p*np.random.rand(4)).astype(int)\n",
    "print(I)\n",
    "w0[I] = np.array([.8, -.6, .7, -.9])\n",
    "N = np.random.randn(n)*np.max(np.abs(B@w0))*.02\n",
    "y = (B@w0) + N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fixe $\\lambda = \\frac{\\lambda_{\\max}}{10}$ où $\\lambda_{\\max} = \\Vert B^\\top y \\Vert_\\infty$ est la valeur limite du paramètre pour laquelle on peut montrer que la solution du problème du Lasso est nulle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = np.max(np.abs(B.T @ y))/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On donne aussi un exemple d'utilisation de la fonction `stem` de matplotlib, utile pour représenter les vecteurs parcimonieux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stem( w0, linefmt='--k', markerfmt='ko', label='$x_0$' )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de re-écrire le problème du Lasso comme un problème d'optimisation lisse sans contrainte, on introduit, pour tout vecteur $w\\in \\mathbb{R}^p$, on  introduit le vecteur $x=(x_-,x_+)$ tel que $x_-=\\max(-w,0)$ et $x_+=\\max(w,0)$. On a ainsi toujours \n",
    "$$w = x_+ - x_-\\quad \\text{et}\\quad |w|=x_++x_-\\quad \\text{et}\\quad x\\ge 0.$$\n",
    "Le problème revient donc à minimiser, sous la contrainte $x\\ge 0$ et en décomposant $x=(x_+,x_-)$\n",
    "$$ \n",
    "    f(x) = \\frac{1}{2}\\Vert B(x_+-x_-)-y\\Vert_2^2 + \\lambda \\langle x,1\\rangle,\n",
    "$$ \n",
    "où $1$ est le vecteur rempli de $1$. Ainsi la contrainte s'écrit bien $Ax \\le b$ avec\n",
    "$A=-\\text{Id}_{2p}$, $b=0$.\n",
    "<div class=\"alert alert-block alert-info\"> Calculez ci-dessous le gradient et la Hessienne de $f$ en fonction de $B,y,\\lambda$ et de $x=(x_-,x_+)$  \n",
    "$$\\nabla f(x) = ?? \\quad H[f](x) = ?? $$\n",
    "</div>\n",
    "\n",
    "\n",
    "Remplissez une classe `class lasso()` qui prend au constructeur `B`, `lam` et `y` qui valent respectivement $B$, $\\lambda$ et $y$. Cette classe doit vous calculer $f(x)$, $\\nabla f(x)$ ainsi que la Hessienne dans les fonctions idoines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lasso() :\n",
    "    def __init__(self,B,lam,y) :\n",
    "        pass\n",
    "    def zeros(self) :\n",
    "        pass\n",
    "    def value(self,x) :\n",
    "        pass\n",
    "    def grad(self,x) :\n",
    "        pass\n",
    "    def Hess(self,x) :\n",
    "        pass\n",
    "f=lasso(B,lam,y)\n",
    "np.random.seed(42)\n",
    "x=np.random.randn(2*p)\n",
    "d=np.random.randn(2*p)\n",
    "print('## TEST DE LA FONCTION##')\n",
    "print(f.value(x))                         # 3576.442886034992\n",
    "print(f.grad(x).shape,f.grad(x)[3:6])     # (120,) [ 247.688236   -146.62731787  -37.53567201]\n",
    "print(f.Hess(x).shape,f.Hess(x)[15][3:6]) # (120, 120) [4.15670644 6.06401088 6.55285271]\n",
    "print('## TEST DE DERIVEE NUMERIQUE##')\n",
    "opt.deriv_num(f,x,d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez les classes `lin_cst` et `lasso` définies au dessus pour résoudre le problème du Lasso avec une méthode de Newton, une précision de $10^{-8}$, en partant du point $x_0=1$ pour les différentes valeurs de `t` données dans le tableau `tlist` ci-dessous. Vous afficherez pour chaque `t` le vecteur $w_0$ et sa reconstruction en utilisant le module `plt.stem` donné plus haut dans le notebook. Que remarquez vous sur la solution ? Pouvez-vous l'expliquer ?\n",
    "<div class=\"alert alert-block alert-info\"> VOTRE REPONSE CI-DESSOUS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlist = np.array([1, 10, 100, 1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez ci-dessous la fonction `central_path`. Vous mettrez une tolérance de $10^{-5}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les différentes valeurs de `lam_list` donnez ci-dessous, lancez l'algorithme d'optimisation `central_path` pour résoudre le problème du Lasso. vous mettrez une tolérance de $10^{-4}$. Que remarquez-vous quand $\\lambda$ var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_list = np.array([0.01*lam, 0.1*lam, lam,4*lam])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
