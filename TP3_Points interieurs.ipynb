{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Méthode de point intérieur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de ce TP est d'utiliser des méthodes de point intérieur pour minimiser une fonction sous contrainte.\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\umin}[1]{\\underset{#1}{\\min}\\;}$\n",
    "$\\DeclareMathOperator{\\eqdef}{\\overset{\\tiny def}{=}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit $f$ une fonction régulière de $\\mathbb{R}^n$ dans $\\mathbb{R}$, nous nous intéresserons à deux types de problème : \n",
    "1. Le problème à contrainte linéaire \n",
    "$$\n",
    "    (\\mathcal{P}_{\\infty,\\text{ lin.}}) \\qquad \\umin{x\\in \\mathbb{R}^n, A x \\leq b} f(x)\n",
    "$$\n",
    "où $A$ est une matrice $\\mathcal M_{pn}(\\mathbb{R})$ et $b\\in \\mathbb{R}^p$.\n",
    "2. Le problème avec **une** contrainte non linéaire\n",
    "$$\n",
    "    (\\mathcal{P}_{\\infty,\\text{ non lin.}}) \\qquad \\umin{x\\in \\mathbb{R}^n, g(x)\\le 0} f(x)\n",
    "$$\n",
    "Où $g$ est une fonction régulière de $\\mathbb{R}^n$ dans $\\mathbb{R}$.\n",
    "\n",
    "\n",
    "L'idée des méthodes de point intérieur est d'approximer les problèmes $(\\mathcal P_{\\infty,\\bullet})$ en utilisant des [fonctions barrières logarithmiques](https://en.wikipedia.org/wiki/Barrier_function)\n",
    "\n",
    "$$\n",
    "\t(\\mathcal{P}_{t, \\text{non-lin.}}) \\qquad \\umin{x\\in \\mathbb{R}^d, } f_t(x) \\eqdef f(x) - \\frac{1}{t} \\text{ln}( -g(x)  )\n",
    "$$\n",
    "ou \n",
    "$$\n",
    "\t(\\mathcal{P}_{t, \\text{ lin.}}) \\qquad \\umin{x\\in \\mathbb{R}^d, } f_t(x) \\eqdef f(x) - \\frac{1}{t} \\text{Log}( b-A x  )\n",
    "$$\n",
    "avec\n",
    "$$\n",
    "\t\\text{Log}(u) \\eqdef \\sum_i \\ln(u_i)\n",
    "$$\n",
    "\n",
    "Ainsi la fonction -Log ou -log qui est une fonction strictement concave, agit comme une barrière pour la contrainte. On s'attend à ce qu'à la limite, quand $t$ tend vers $+\\infty$, le problème $(\\mathcal P_{t,\\bullet})$ tende vers le problème $(\\mathcal P_{\\infty,\\bullet})$.\n",
    "## I-Contrainte non-linéaire\n",
    "On suppose dans cette section que l'on se place dans le cadre d'une seule contrainte non-linéaire.\n",
    "Si $f$ et $g$ sont données et $f_t=f - \\frac{1}{t} \\text{log}( -g)$, \n",
    "  \n",
    "<div class=\"alert alert-block alert-info\"> Calculez ci-dessous le gradient et la Hessienne de $f_t$ en fonction des gradients et de la Hessienne de $f$ et de $g$ \n",
    "$$\\nabla f_t(x) = ?? \\quad H [f_t](x) = ?? $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On suppose que `f` et `g` sont des fonctions définies avec une classe dans le fichier `functions.py` (ainsi les calculs du gradient ou de la Hessienne de `f` et de `g` sont déjà faits). Créer ci-dessous une classe `non_lin_cst` qui calcule `f_t`, son gradient ou sa Hessienne. La classe prend à la construction la fonction `f`, la fonction `g` et la valeur de `t`. Le constructeur de la classe est déjà implémenté.\n",
    "Attention, dans le cas où `g(x)` est $>0$, la fonction `value(self,x)` doit rendre `np.inf` et ne doit pas rendre une erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import functions as func\n",
    "import Optim as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class non_lin_cst() :\n",
    "    def __init__(self,f,g,t) :\n",
    "        self.zeros()\n",
    "        self.f=f\n",
    "        self.g=g\n",
    "        self.t=t\n",
    "        self.nb_constraints=1. # nombre de contraintes du problème\n",
    "    def zeros(self) :\n",
    "        self.nb_eval=0 # number of evaluations of the function self.value()\n",
    "        self.nb_grad=0 # number of evaluations of the function self.grad()\n",
    "        self.nb_hess=0 # number of evaluations of the function self.Hess()\n",
    "    def value(self,x) :\n",
    "        if self.g.value(x) > 0:\n",
    "            return np.inf\n",
    "        else:\n",
    "            self.nb_eval += 1\n",
    "            return self.f.value(x) - (1/self.t)*np.log(-self.g.value(x))\n",
    "        return None\n",
    "    def grad(self,x) :\n",
    "        self.nb_grad += 1\n",
    "        return self.f.grad(x) - (1/self.t)*self.g.grad(x)/self.g.value(x)\n",
    "    def Hess(self,x) :\n",
    "        self.nb_hess += 1\n",
    "        grad_g = self.g.grad(x).reshape(-1,1)\n",
    "        return self.f.Hess(x) + (1/self.t)*(-self.g.Hess(x)/self.g.value(x) +  grad_g@grad_g.T  /self.g.value(x)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de tester notre classe, on utilise comme `f` la fonction Rosenbrock et comme `g` la fonction `square2` définie ci-dessous :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doit être 1.0= 1.0\n",
      "Doit être 1.5012148269226977= 1.5012148269226977\n",
      "Doit être inf = inf\n",
      "######## TEST DE DERIVEE NUMERIQUE\n",
      "eps 1.0e-01 grad 1.2e+00 ratio 9.5e-02 angle 1.2e-03\n",
      "eps 1.0e-02 grad 1.1e-01 ratio 5.1e-03 angle 2.8e-05\n",
      "eps 1.0e-03 grad 1.1e-02 ratio 4.9e-04 angle 3.0e-07\n",
      "eps 1.0e-04 grad 1.1e-03 ratio 4.8e-05 angle 3.0e-09\n",
      "eps 1.0e-05 grad 1.1e-04 ratio 4.8e-06 angle 3.0e-11\n",
      "eps 1.0e-06 grad 1.1e-05 ratio 4.8e-07 angle 3.0e-13\n",
      "eps 1.0e-07 grad 1.1e-06 ratio 4.8e-08 angle 3.0e-15\n",
      "eps 1.0e-08 grad 1.1e-07 ratio 5.8e-09 angle 3.3e-16\n",
      "eps 1.0e-09 grad 2.0e-10 ratio 6.3e-10 angle 1.1e-16\n",
      "eps 1.0e-10 grad 1.1e-07 ratio 2.1e-08 angle 5.6e-16\n",
      "eps 1.0e-11 grad 1.1e-07 ratio 9.4e-07 angle 1.7e-14\n",
      "eps 1.0e-12 grad 3.1e-06 ratio 6.0e-06 angle 9.7e-15\n"
     ]
    }
   ],
   "source": [
    "class square2() :\n",
    "    def __init__(self) :\n",
    "        self.zeros()\n",
    "    def zeros(self) :\n",
    "        self.nb_eval=0 # number of evaluations of the function self.value()\n",
    "        self.nb_grad=0 # number of evaluations of the function self.grad()\n",
    "        self.nb_hess=0 # number of evaluations of the function self.Hess()\n",
    "    def value(self,x) :\n",
    "        # returns the value of the function at point x\n",
    "        self.nb_eval+=1\n",
    "        return 0.5*x[0]**2+7/2.*x[1]**2-1\n",
    "    def grad(self,x) :\n",
    "        # returns the gradient of the function at point x\n",
    "        self.nb_grad+=1\n",
    "        return np.array([x[0],7*x[1]])\n",
    "    def Hess(self,x) :\n",
    "        # returns the Hessian of the function at point x\n",
    "        self.nb_hess+=1\n",
    "        to_return=np.zeros((2,2))\n",
    "        to_return[0,0]=1\n",
    "        to_return[1,1]=7\n",
    "        return to_return\n",
    "f_t=non_lin_cst(func.Rosen(),square2(),0.33)\n",
    "x_0=np.array([0.,0.])\n",
    "print('Doit être 1.0=',f_t.value(x_0))\n",
    "x_0=np.array([0.2,0.12])\n",
    "print('Doit être 1.5012148269226977=',f_t.value(x_0))\n",
    "x_0=np.array([2,1.3])\n",
    "print('Doit être inf =',f_t.value(x_0))\n",
    "print('######## TEST DE DERIVEE NUMERIQUE')\n",
    "a=np.array([0.2,0.12])\n",
    "d=np.random.randn(2)\n",
    "opt.deriv_num(f_t,a,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme d'optimisation\n",
    "Vous devez récupérer votre algorithme de Newton ainsi que la recherche linéaire de Wolfe (avec un step initial de $1$ ) dans la cellule ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_wolfe(x,function,step,descent,f,df) :\n",
    "    e1 = 1e-7\n",
    "    e2 = 0.9\n",
    "    sl = 0\n",
    "    sh = 10000\n",
    "    step2 = step\n",
    "    while True:\n",
    "        if function.value(x + step2*descent) > f + e1*step2*np.dot(df,descent):\n",
    "            sh = step2\n",
    "            step2 = (sl + sh)/2\n",
    "        elif np.dot(function.grad(x+step2*descent),descent) < e2*np.dot(df,descent):\n",
    "            sl = step2\n",
    "            if sh < 10000:\n",
    "                step2 = (sl + sh)/2\n",
    "            else:\n",
    "                step2 = 2*step2\n",
    "        else:\n",
    "            x2 = step2*descent + x\n",
    "            f2 = function.value(x2)\n",
    "            df2 = function.grad(x2)\n",
    "            return x2,f2,df2,step2\n",
    "    \n",
    "def ls_wolfe_step_is_one(x,function,step,descent,f,df) :\n",
    "    return ls_wolfe(x,function,1.,descent,f,df)\n",
    "\n",
    "    \n",
    "def dc_Newton(x,function,df) :\n",
    "    descent = np.matmul(np.linalg.inv(function.Hess(x)),-df)\n",
    "    c = np.dot(descent,-df)/(np.linalg.norm(descent)*np.linalg.norm(df))\n",
    "    if c > 0.1:\n",
    "        return descent\n",
    "    else:\n",
    "        return -df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant lancer une optimisation en utilisant la méthode de Newton avec un pas de Wolfe initialisé à $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=   0 f=1.000e+00 df=2.000e+00 comp=[   1,   1,   0]\n",
      "iter=   1 f=8.586e-01 df=8.192e+00 comp=[   4,   3,   1]\n",
      "iter=   2 f=7.028e-01 df=3.443e-01 comp=[   6,   5,   2]\n",
      "iter=   3 f=6.973e-01 df=2.196e-01 comp=[   8,   7,   3]\n",
      "iter=   4 f=6.972e-01 df=1.166e-03 comp=[  10,   9,   4]\n",
      "iter=   5 f=6.972e-01 df=6.426e-07 comp=[  12,  11,   5]\n",
      "iter=   6 f=6.972e-01 df=3.808e-14 comp=[  14,  13,   6]\n",
      "Success !!! Algorithm converged !!!\n",
      "x final= [0.25636244 0.05915168]\n"
     ]
    }
   ],
   "source": [
    "f_t=non_lin_cst(func.Rosen(),square2(),0.33)\n",
    "res=opt.main_algorithm(f_t,5,np.array([0,0]),dc_Newton,ls_wolfe_step_is_one,tol=1.e-7,verbose=True)\n",
    "final_x=res['list_x'][-1]\n",
    "print('x final=',final_x)\n",
    "# VOUS DEVEZ TROUVER\n",
    "#Fonction de Rosenbrock\n",
    "#Fonction (x,y) --> x^2/2+7/2*y^2-1\n",
    "#Fonction Interior point method\n",
    "#iter=   0 f=1.000e+00 df=2.000e+00 comp=[   1,   1,   0]\n",
    "#iter=   1 f=8.586e-01 df=8.192e+00 comp=[   3,   2,   1]\n",
    "#iter=   2 f=7.028e-01 df=3.443e-01 comp=[   4,   3,   2]\n",
    "#iter=   3 f=6.973e-01 df=2.196e-01 comp=[   5,   4,   3]\n",
    "#iter=   4 f=6.972e-01 df=1.166e-03 comp=[   6,   5,   4]\n",
    "#iter=   5 f=6.972e-01 df=6.426e-07 comp=[   7,   6,   5]\n",
    "#iter=   6 f=6.972e-01 df=3.988e-14 comp=[   8,   7,   6]\n",
    "#Success !!! Algorithm converged !!!\n",
    "#x final= [0.25636244 0.05915168]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La méthode du chemin central\n",
    "Notre objectif est d'envoyer le paramètre $t$ vers $+\\infty$. Pour ce faire on va suivre la méthode du chemin central, nous commençons par nous donner $t_1$ et nous notons $x_0$ le point initial et $x_1$ le minimiseur donné de $f_{t_1}$ par une méthode de Newton commençant à $x_0$. Ensuite nous multiplions $t_1$ par $\\mu$ pour obtenir $t_2$ et nous lançons une méthode de Newton commençant à $x_1$. On note $x_2$ le résultat obtenu. \n",
    "Ainsi nous allons construire une suite de points solutions du problème \n",
    "$$x_k \\in \\textrm{argmin} f_{t_k}(x).$$\n",
    "avec $t_{k}=\\mu t_{k-1}$ et $x_{k}$ une solution donnée par un algorithme de Newton (avec recherche de Wolfe) et avec comme point de départ $x_{k-1}$. L'idée d'utiliser comme initialisation la solution d'un problème d'optimisation s'appelle \"warm restart\".\n",
    "\n",
    "On peut montrer que l'erreur que l'on fait entre $x_k$ et $x^\\star$ (où $x^\\star$ est le minimum de $f$) vérifie :\n",
    "$$ 0 \\le f(x_k)-f(x^\\star)\\le \\frac{p}{t_k},$$\n",
    "où $p$ est le nombre de contraintes. Ainsi on va s'arrêter dès que $\\varepsilon t_k > p$ où $\\varepsilon$ est la précision de l'algorithme de Newton.\n",
    "Ecrire une fonction `def central_path(function,mu,varepsilon,x0)` qui implémente cette stratégie. Elle prend en argument \n",
    " 1. `function`: une instance de la classe `non_lin_cst`\n",
    " 2. `mu`: un réel $>1$ qui est le facteur multiplicatif de `t`\n",
    " 3. `varepsilon`: une précision\n",
    " 4. `x0`: un point de départ de la méthode\n",
    " \n",
    "Cette fonction doit rendre `costs,x` où\n",
    "\n",
    "1. `costs` est la liste des coûts pour toutes les itérations\n",
    "2. `x` est le point final d'arrivée\n",
    "\n",
    "On rappelle que la valeur de $p$ se trouve dans `function.nb_constraints` tandis que la valeur de $t$ se trouve dans `function.t`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimal value  0.09946170477872637\n",
      "x [0.68525434 0.46758139]\n"
     ]
    }
   ],
   "source": [
    "def central_path(function,mu,varepsilon,x0) :\n",
    "    x = x0\n",
    "    costs = []\n",
    "    while varepsilon*function.t <= function.nb_constraints:\n",
    "        res = opt.main_algorithm(function,5,x,dc_Newton,ls_wolfe_step_is_one,tol=1.e-4,verbose=False)\n",
    "        x=res['final_x']\n",
    "        function.t = mu*function.t\n",
    "        costs += res['list_costs']\n",
    "    return costs, x\n",
    "\n",
    "f_t=non_lin_cst(func.Rosen(),square2(),10)\n",
    "costs,x=central_path(f_t,10,1.e-9,np.array([0,0]))\n",
    "cost_min=np.min(costs)\n",
    "print('minimal value ',cost_min) # minimal value  0.0994618583079908\n",
    "print('x',x) # x [0.68525439 0.46758135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmMAAAGHCAYAAAD89VV0AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAASnVJREFUeJzt3Q98U/W9//FPkqYthRaoQPnXVmSiQAHlrwVxgAJS5Z8ycToEBt5xmSJjXi+MOxXuFDeV8dtVmOgm6qbjOgfuaid0UwFFVECmgDjQavlTqC1/Slto0+T8Ht9vmtD/TWn+nryej8chycnJyUm+Sfrm++9YDMMwBAAAACFhDc3TAgAAQCGMAQAAhBBhDAAAIIQIYwAAACFEGAMAAAghwhgAAEAIEcYAAABCiDAGAAAQQoQxAACAECKMASa0bt06sVgsDS7vvvtuwJ770ksvlVmzZl3UY19++WVZtWpVvfep43744YclnP3jH/+QwYMHS+vWrfXxbty4sd7tjh07pl/Lnj176tyn3rs2bdqI2WRnZ4d9+QGhEhOyZwYQcM8//7xceeWVddb36dMnLN99Fcb27t0rCxcurHPfBx98IN27d5dwpc4sd9ttt0mvXr3kr3/9qw5kV1xxRYNhbNmyZTq4XnXVVRINVBh7+umnCWRAPQhjgIllZGTomhozuOaaayScqYB18uRJmTp1qlx//fWhPhwAEYRmSiCKXX311TJy5Mg6651Op3Tr1k1uueUW7zoVNObPn6/Xx8bGymWXXSZLly6V8vJyn5pMv/766xrrVVNp9SbTUaNGyZtvvinffPNNjSbVxpopVS3a5MmTpX379hIfH69rmV544YV6n+eVV17Rx9u1a1dJSkqSG264Qb744guf3qf33ntPB6zExERJSEiQ4cOH62P1UMflqbX7z//8T/18qtarPup4hgwZoq/Pnj3b+zprv7ZDhw5JVlaWbrJMTU2Vn/70p3Xe64qKCvnFL36haz/j4uKkY8eOep/ffvutT6/rww8/lIkTJ8oll1yi37+ePXvWqZVs6rUrZWVlcv/990uPHj30fpKTk/V/AtR77ml6VbViSvWyrf2ZAKIVYQwwMRWqKisrayxqnYf6w63+2B48eLDG4zZv3qxretT9yvnz52X06NHy4osvyqJFi/Qf4x/84Afyq1/9qkZga4nVq1fLiBEjpHPnzrpJ0rM0RAUpFQz27dsnv/nNb+Qvf/mLbn5Vf/jVcdX2s5/9TAe95557TtauXatfswoi1d+P+mzZskXGjBkjZ86ckd/97nc6YKhgoh67fv16vc3cuXP18yv33nuvPu4NGzbUu7+BAwfq5mPlv/7rv7yvU+3Dw+FwyKRJk3QIev311+WHP/yh/PrXv5Zf/vKX3m1cLpcOoo899pjccccdukzU9ZycHB1sz5071+jr2rRpkw7ieXl5snLlSvnb3/6mj+fEiRPNeu2K+kysWbNGFixYIG+99Za89NJL8r3vfU+Kior0/T//+c9l2rRp+nr1su3SpUujxwhEDQOA6Tz//POG+nrXt9hsNu92hYWFRmxsrPGzn/2sxuNvu+02IyUlxXA4HPr2b3/7W/3Y//3f/62x3S9/+Uu9fvPmzd516enpxsyZM+scS25ubo3HvvPOO3q9uvS46aab9OPro7Z96KGHvLdvv/12Iy4uzsjLy6ux3YQJE4yEhATj9OnTNZ4nKyurxnbqtaj1H3zwQaPv5TXXXGN06tTJOHv2rHddZWWlkZGRYXTv3t1wuVx6nXp9an+PP/640ZSPP/5Yb6vem9rUe1ffe62O/4orrvDefuWVV/R2r732Wr37Xr16daPH0LNnT72cO3euxa9d3Z4yZUqjz/fjH/9YHxeAuqgZA0xM1WR9/PHHNRbVNOWhmqdULYdq2lM1LcqpU6d0bcxdd90lMTHubqVvv/227pDuqd3w8IyaVKMIg00dk6o5Uk14tY9JNZvVrlVTNU3V9e/fX1+q2rKGlJaW6vdLve7qIxxtNpvMmDFDjhw54nNTZ3OoJjxVLrWPt/qxvvHGG9KuXTu9XfWaT9VUq2oXGxsx+69//Uu+/PJLmTNnjm5WbOlrHzp0qK5ZW7x4sX7epmrlANREB37AxHr37t1kB37VBPbaa6/p5q3x48frpijVN6n69BSquUn9ga/eh0vp1KmTDmye5qhgUs9ZXzOX6hPmub86FTyrU32slMaCgwqmqlKuOc/jD6pvVu2QpI5XNRd7qObE06dP6/579SksLGxw/54+ZY2NTm3Oa1fNxGpfqulSNaWqY1efpccff1wuv/zyJl8vEO0IY0CUU3801R9X1Y9JXVeXw4YNqzH9hQoyqpZE/XGuHsgKCgp0bUyHDh0a3L8nVNTufN5YWPCFOqb8/Pw661VfN6WxY/KVGhhgtVoD/jwXQz2veg9UH636qL5dDVEd/RVVu+WP165qTdVUHWpRIdFTS6Zq7Q4cONDs1wZEG5opgSjnaXZSE5Ru27ZNdu7cqWvLqlPNgSUlJXUmMVXNoJ77G+IZVfjpp5/WWK/m4qpN1f742sSlnlM1VXqCQfVjUjVL/pgKQ4UMFUxV5/zqx6WadP/whz/o2iA1r1hz+VIr15Sbb75Z10ypAQiq9rP20tAcZ4o6ZjVy8ve//32Do2Ev9rWnpKToWtXvf//7uhlTNRn76zUDZkXNGGBiauoHVXNVm/pD7KkdUVT4Us1LalReq1atZPr06TW2V/3H1NQEM2fO1NMR9OvXT4/CfPTRR/X0C2qaiIaoaRxUMFBTH6hjUTUuaqShenxtar/qj78amTdo0CBdM9NQM+tDDz2k+02pUZ4PPvignk7hj3/8ox5VqEZTtm3bVvxhxYoVMnbsWP086jWoZkE18lO9t6pJt3bTrS/U+6/eZ3W8qilZ9clStZOe5j9f3H777frx6v2/7777dL8tu92ua7veeecdPdJSzXnWEFWequZKhdaf/OQnkpaWpkdWqlGWar/Nee0qtKlwqPq1qfL9/PPP9YjKzMxMHYw9Zauoz9mECRP0fwLU9g01swJRpZ5O/QBMPJpSLc8++2ydxwwfPlzfd+edd9a7z6KiImPevHlGly5djJiYGD3qccmSJcb58+drbFd7NKXyr3/9yxg3bpyRlJRkdOzY0bj33nuNN998s85oypMnTxrTpk0z2rVrZ1gslhqj72qPplQ+++wzY+LEiUbbtm31qNABAwbUGaHoGU356quv1ljvGf1Y34jG2rZt22aMGTPGaN26tdGqVSs9yvD//u//6t2fL6MpPaMhr7zySsNut9d4beq9U89Tm7q/9k+2Gu36xBNP6NcdHx9vtGnTRu/zRz/6kXHw4MEmj0GNJFWjT9X7p0amqtGVP/nJT5r92hcvXmwMHjzYaN++vd7PZZddpvejRut6lJeXG3PnztXl7ynb2iNsgWhlUf+EOhACAABEK/qMAQAAhBBhDAAAIIQIYwAAACFEGAMAAAghwhgAAEAIEcYAAABCKGomfVWzRquZutUpQi5mkkYAAIDmULOHnT17Vk/orCaxlmgPYyqIpaamhvowAABAlDl8+LA+hZhEexjznDRXvSFJSUkBeQ6HwyGbN2+WcePG6dOSIDJRjuZAOZoHZWkO0ViOxcXFuiLIk0Ek2sOYp2lSBbFAhjF1Hja1/2j5oJkR5WgOlKN5UJbmEM3laGmiexQd+AEAAEKIMAYAABBChDEAAIAQIowBAACEUESFsTfeeEOuuOIKufzyy+W5554L9eEAAAC0WMSMpqysrJRFixbJO++8o0diDBw4UG655RZJTk4O9aEBAACYv2bso48+kr59+0q3bt30fB1ZWVmyadOmUB8WAABAZISxrVu3ysSJE/UpAdR8Gxs3bqyzzerVq6VHjx4SHx8vgwYNkm3bttWYQV8FMQ81k+3Ro0eDdfgAAACRHcZKS0tlwIAB8tRTT9V7//r162XhwoWydOlS+eSTT2TkyJEyYcIEycvL857fqTbOMQkAACJd0PqMqWClloasXLlS5syZI3PnztW3V61apZsh16xZIytWrNC1YtVrwo4cOSLDhg1rcH/l5eV6qX5KAs8MwGrxNxUW//rkj+XMiXwpGJghnTqn+f05EByez0cgPicIHsrRPChLc4jGcnT4+FrDogN/RUWF7Nq1SxYvXlxjvTp/1fbt2/X1oUOHyt69e3UgUx34s7Oz5cEHH2xwnyrALVu2rM56dV4sdTqGQLj8D+9JrFMk5zsvS/vU/gF5DgRPTk4Ob7cJUI7mQVmaQzSVY1lZWeSEscLCQnE6nZKSklJjvbp9/PhxfT0mJkaefPJJGT16tLhcLnnggQfkkksuaXCfS5Ys0aMva5+sUwW8QJ2b8p8PLdZh7LLUzpKZlRWQ50Bw/iejfizGjh0bdedPMxPK0TwoS3OIxnIsrmqVi4gw1lAfMNX0V33dpEmT9OKLuLg4vdSmPgCB+hC4PIdqGFHzQTOzQH5WEDyUo3lQluYQTeVo9/F1hsXUFh06dBCbzeatBfMoKCioU1sWzoyqMGY4o6c9HAAAtExYhLHY2Fg9lUXtdmR1e/jw4RJpYUw1uQIAAIRVM2VJSYkcOnTIezs3N1f27NmjZ9BPS0vT/btmzJghgwcPlszMTFm7dq2e1mLevHkSKTyTbxiuyhAfCQAAiBRBC2M7d+7Une89PJ3rZ86cKevWrZPp06dLUVGRLF++XPLz8yUjI0OPmExPT5dIqxlzVdJMCQAAwiyMjRo1qt6JW6ubP3++XiLVhT5jNFMCAIAI6jNmFt6aMSfNlAAAwDeEsYCEMWrGAACAbwhjgWimdBHGAACAbwhjAekzRjMlAADwDWEsEPOMUTMGAAB8RBjzI89Y0dJzFf7cLQAAMDHCWABqxmJ5VwEAgI+IDX5EnzEAANBchLEAhLG484X+3C0AADAxwpg/eTrwi92vuwUAAOZFGPMjl8U70Zg/dwsAAEyMMOZHRlUYMwwmfQUAAL4hjAWgz5hwOiQAAOAjwlggRlManhnHAAAAGkcY8yPOTQkAAJqLMBaAPmMWOvADAADCWOjEni3nAwgAAHxCzZgfdfnWPYrSEWfz524BAICJEcb8KK9rjPuKi6ktAACAbwhjAegzxqSvAADAV4SxAIymjC8/48/dAgAAE4uoMHb48GEZNWqU9OnTR/r37y+vvvqqhBWL+zRI5baEUB8JAACIEFWdnCJDTEyMrFq1Sq666iopKCiQgQMHSlZWlrRu3VrCgcuqOu47RVxM+goAAEwYxrp06aIXpVOnTpKcnCwnT54MmzB2YZ4xwhgAAAhBM+XWrVtl4sSJ0rVrV7FYLLJx48Y626xevVp69Ogh8fHxMmjQINm2bdtFPdfOnTvF5XJJamqqhAuX59yULndzJQAAQFBrxkpLS2XAgAEye/ZsufXWW+vcv379elm4cKEOZCNGjJBnnnlGJkyYIPv375e0tDS9jQpo5eV1J03dvHmzDnlKUVGR3HXXXfLcc881eCxqH9X3U1xcrC8dDodeAqKqZqxUAvgcCDhP2VGGkY1yNA/K0hyisRwdPr5WixGgs1qrmrENGzbIlClTvOuGDRum+3mtWbPGu6537956mxUrVvi0XxWwxo4dK3fffbfMmDGjwe0efvhhWbZsWZ31L7/8siQkBKaD/ekXfi5D9zvko+EJ0m7ygwF5DgAAEBnKysrkjjvukDNnzkhSUlLo+4xVVFTIrl27ZPHixTXWjxs3TrZv3+7TPlRunDVrlowZM6bRIKYsWbJEFi1aVKNmTDVpqudr7A1piY0v/bfKwWK3WvXAAkTu/2RycnJ06Lfb7aE+HFwkytE8KEtziMZyLK5qlWtK0MJYYWGhOJ1OSUlJqbFe3T5+/LhP+3j//fd1U6ea1sLTH+2ll16Sfv361dk2Li5OL7WpD0CgPgRWS9VpkAz38yCyBfKzguChHM2DsjSHaCpHu4+vM+ijKVXzZe3artrrGnLttdfqTvthy+p+HfbSylAfCQAAiBBBm/S1Q4cOYrPZ6tSCqfnCateWRSp7VQZrVRLGgREAAERnGIuNjdUjJVV7cXXq9vDhw8UMXLHut7Mi3reaPgAAAL82U5aUlMihQ4e8t3Nzc2XPnj16clY1dYXqUK863g8ePFgyMzNl7dq1kpeXJ/PmzTNFSZS1q+qjxqSvAAAgFGFMTcQ6evRo723PaMaZM2fKunXrZPr06XqOsOXLl0t+fr5kZGRIdna2pKeniylY3TVjzMAPAABCEsbUSbybmrZs/vz5ejEjS1UH/njXuVAfCgAAiBBB6zMWTRxGbKgPAQAARAjCmB8Z9lb6kmZKAADgK8KYHxlVfcbUpK8AAAC+IIz5kcXqnoHfQhgDAAA+Ioz5kaWqZsygagwAAPiIMBaAZspi3lUAAOAjYoMfGVXvZmunP/cKAADMjDDmR21j27qv0GcMAAD4iDDmTxbPDPx+3SsAADAxwpgfWWzu0ZQ98tTpKUlkAACgaYQxP4pt45701e4UyTtZ5s9dAwAAkyKM+ZF9wGXe6+UOevEDAICmEcb8yGq3e687Kyv9uWsAAGBShDF/vpnWGO91w0mfMQAA0DTCmD/ZLoSxxC9e9euuAQCAORHG/MiW1M17Pb5grz93DQAATIow5keWan3GXC76jAEAgKYRxgLUZ0xcLn/uGgAAmBRhzJ9vZtWkr5qTmjEAANA0wpgfWa0XwphBzRgAADBjGCsrK5P09HS5//77JazDWCWTvgIAABOGsUceeUSGDRsW6sOo1yWtOojL4r5eSQd+AABgtjB28OBBOXDggGRlZUk4SoxN9IYxw6ADPwAACGIY27p1q0ycOFG6du0qFotFNm7cWGeb1atXS48ePSQ+Pl4GDRok27Zta9ZzqKbJFStWSLiyWqxiVIUxl5MwBgAAghjGSktLZcCAAfLUU0/Ve//69etl4cKFsnTpUvnkk09k5MiRMmHCBMnLy/NuowJaRkZGneXYsWPy+uuvS69evfQSrixi8daMOWmmBAAAPqg2MVbLqGClloasXLlS5syZI3PnztW3V61aJZs2bZI1a9Z4a7t27drV4ON37Nghf/rTn+TVV1+VkpIScTgckpSUJA8++GC925eXl+vFo7i4WF+qx6klEJyVTrFWnZLy9L9OBux5EFiecqP8IhvlaB6UpTlEYzk6fHytFsMw/H5Ga9VMuWHDBpkyZYq+XVFRIQkJCTpITZ061bvdfffdJ3v27JEtW7Y0a//r1q2TvXv3yhNPPNHgNg8//LAsW7aszvqXX35ZH0sgOA2n9F68VF8vvqatHJ+6JCDPAwAAImMGiDvuuEPOnDmjK5ACXjPWmMLCQnE6nZKSklJjvbp9/PjxgDznkiVLZNGiRTVqxlJTU2XcuHGNviEtUV5RLq/9/r9k6L8MscfGhO1AAzT9P5mcnBwZO3as2Kud4gqRhXI0D8rSHKKxHIurWuWaEpQwVr3GrDpVKVd7nS9mzZrV5DZxcXF6qU19AAL1IVCvx9OBXwwjaj5sZhXIzwqCh3I0D8rSHKKpHO0+vs6gTG3RoUMHsdlsdWrBCgoK6tSWRTIVLD0d+Due+1rkzNFQHxIAAAhzQQljsbGxeqSkqp6sTt0ePny4mIl3agvVE++jZ0J9OAAAIMz5rZlSjXA8dOiQ93Zubq7unJ+cnCxpaWm6/9aMGTNk8ODBkpmZKWvXrtXTWsybN0/MxKWbXQ0xVCqrKA314QAAgGgJYzt37pTRo0d7b3s6z8+cOVOPfpw+fboUFRXJ8uXLJT8/X88flp2drc8zacaaMT1ElVn4AQBAsMLYqFGjdAf2xsyfP18vZuauGdP99wljAADAXOemjASGN4xZ1DmRQn04AAAgzBHGAtRM+aXdLmecFf7ePQAAMBnCmJ8ZFvdb+n6rVrKgbJ+/dw8AAEyGMOZnbY0e7jfWJXLcdeHcmAAAAPUhjPlZvCVZX1oMQ1zuMZUAAAANIoz5W1UH/linEMYAAEB4nZsyGrhat9aXY3cb0sZRIa/vWCLW9Mtk/E/nSGwM2RcAANREGPOz2NHXyJkvPpC2hSVy/S41tcVGvf69gVfJmLFD/P10AAAgwlFV42eWxNbSbvlU+X+TrPLWUKucs8fr9edOFfv7qQAAgAkQxgLAFhMj7/e1ymvX26U8oY1eZzABLAAAqAdhLACsFpu+VKMpPfOOiYuRlQAAoC7CWABYrVVhzOWUJNdpfT2m/FQgngoAAEQ4wlgg3tRW7b01Y62MUn095cT7gXgqAAAQ4QhjgXDpSH1h2GLFYY1zv9FOZuMHAACEsaCwWu360mWxisMa617pcvH5AwAAdVAzFgDWqk77LsPlnZGfMAYAAOpDGAsAS1UAc6leY4QxAADQCGbgDwCbZ2oLwyXnrS5RjZbfGKflXP6OBh9jt9qlf4f+Yre5mzgBAEB0IIwFMIwpJ2znJVFE/mwclE82393o46b1miYPZT4UiEMCAABhijAWAG3j2srtV9wuuwp2id3yhW6w7OhqJZe3T6t3+4OnDurLo2ePBuJwAABAGCOMBcjSa5bqy09XDhWRszLD+R25atKf6t02+6ts+c9t/6n7mAEAgOgSUR34c3NzZfTo0dKnTx/p16+flJa6J1QNZ54O/BY1srKJ0ZeGwSmTAACINhFVMzZr1iz5xS9+ISNHjpSTJ09KXJx7QtWwVhXGKg6ckLd+8T/1bnLi/Jcyvtgl7e3fyFsf/8+F6TDqExsr/b4/Rbp17xSoIwYAAEEUMWFs3759YrfbdRBTkpOTJSLEuDvzt/miQNp8sbreTdJFRDVmiqg+Y/VvU93f9+yTmX/8f34+UAAAENFhbOvWrfL444/Lrl27JD8/XzZs2CBTpkypsc3q1av1Nur+vn37yqpVq7zhqikHDx6UNm3ayKRJk+TIkSMybdo0+dnPfibhrvPE/lL5f9lSENNdDsddXu82FXJSio1DEiNtpJ2ld4P7Svz2mHQuPCIxZ88E8IgBAEBEhjHVf2vAgAEye/ZsufXWW+vcv379elm4cKEOZCNGjJBnnnlGJkyYIPv375e0NPcow0GDBkl5ed1zOG7evFkcDods27ZN9uzZI506dZIbb7xRhgwZImPHjpVwlpKRKlJ0WroN+Z5cfdMT9W7zdt7bsuyd+2RAx17yh6wXGtzXx0+sFnnuf9QEZgE8YgAAEJFhTAUrtTRk5cqVMmfOHJk7d66+rWrFNm3aJGvWrJEVK1bodapWrSHdu3fX4Ss1NVXfzsrK0sGsoTCmQl31YFdcXKwvVahTSyB49lt9/1aXIaqh0umsFFcDz+tyujv3O13ORo/NOwTAcAXsNaD+ckTkoRzNg7I0h2gsR4ePrzUofcYqKip00Fq8eHGN9ePGjZPt27f7tA8VxE6cOCGnTp2Stm3b6mbRH/3oRw1urwLesmXL6q1lS0hIkEDKycnxXu91/JCohse8b76WT7Oz693+C4eai0zk1OlTkt3ANkp5Xp70U6GtoqLR7eAf1csRkYtyNA/K0hyiqRzLysrCJ4wVFhaK0+mUlJSUGuvV7ePHj/u0j5iYGHn00Ufluuuu01NAqCB38803N7j9kiVLZNGiRTVqxlStmnpcUlKSBCoBqw+Zqq1Tgw0U63sHRPJFLi3fL+kn19b7uGQ5Jy9ZRZIqC2TiqWcb3H++46So+r2BckD6NLJdDXFJ4rxhuUg7NUwAF1uOiDyUo3lQluYQjeVYXNUqF1ajKT0n0PZQoar2upY0hVanpr2ob+oL9QEI9IegxnO0664vLCUn9FIfW6t4kc6dxKgoFeuxbQ3uN+FsghRLO2nrOiPWr7/2+Xis3QaKXHd/c19G1AvGZwWBRzmaB2VpDtFUjnYfX2dQwliHDh3EZrPVqQUrKCioU1tmOv2niySmiJw73eAmluKvRA69JK52qSKZjzS43bcbt4vs/Jt8Yf2OpN46r+nn3v2iSO4W1SntYo8eAAAEWFDCWGxsrB4pqaonp06d6l2vbk+ePFlMzRYj8p0bGt3Emv+hO4zFJYr0m9bgdmUfnJM4+ZsUSftGt/P65n13GBNGXwIAYPowVlJSIocOHapx6iI12lFNzqqmrlD9t2bMmCGDBw+WzMxMWbt2reTl5cm8eT7U8Jicr6dDslo9p1YiXAEAYBZ+C2M7d+7U54308HSenzlzpqxbt06mT58uRUVFsnz5cj3pa0ZGhh4RmJ5Ox3KLuENWUycKt1irTiXqcxir6o9HeAMAwPxhbNSoUU3W7MyfP18vqL9mrLSiVLYcVs2K9Ttb+qX0VFcsZxvdzqviWxE1OODcURFftodW6ayUA44D0uZoG4lRzcyI2nLs37G/tI9v7/djA4Dq+EsTBmKs7mIoOFcg97x9T4PbjTjskvtURVfMkUa3q6FzJ5Fvt4i8TRhrrj9s+UOzHwNzleOVyVfKqxNf9evxAEBthLEw0PuS3nLTZTdJXnFeo9ulJBSp6WPF4oqTfh0aPoel1+lvREq+FUnq6l7gE1XDe/r0aWnXrl2zpl6BecrxvPO8HDx1UI6X+jYPIgC0BGEsDNitdnls5GNNbvd58V9EZKnYylPl5ZtebnrHb94vcvBZkctnioxZ6p+DjZKJCVV/xqzxWVEzF44ZtaQcvzr9lUx+3eQjvQGEjaoe4YgE1qoO/Baj8Y7+AAAgclAzFkE8oynbl56Wtx59usnt0/P/JZ1PJsjR03vkyI6mt4eby+WSwmPHJOezPG8ARnSVY2nlKRlb5JJYa5m89RXfnUj/TsZfdplcd9uN3umBgHBDGIsg9oRWoubS71xaJPLiUz495ri0E5vslXTZG/DjM5MeoT4AhLwc++h/1Ul+ffuuIby/kx+kpsqIERl+OhrAvwhjEST9huskZ/REqcj3rVNx9/JD0tFxTI7Hpkl+7KUBPz6zUDO0nC8/L/Fx8UL//egsx0o5L6eNvWIRm1xiuTpQh4gglGXawT0S63TI2YJC3m+ELcJYBLHGxcn4Nb/y/QHZD4h89Ix0HzlH5PqfB/LQTNnxe0IWHfijtRy/OvOVTN44WdrGtZX3bn8xYMeIwJflh4OHS2zJKd5qhDU6xJiZ97+QnD4JAIBwRRgDAAAIIcKYqXFuSgAAwh1hDAAa0NT5dgHAHwhjZkafMQAAwh5hDABqsXia+GEaVHIinBHGTI0+YwAAhDvCGADAvJi5GRGAMGZm9BkDWsRgjj4AQUAYAwAACCHCWDSg5yrQLHTgBxBMhDEAAIAQIoyZGX3GAAAIexEVxn79619L3759pU+fPrJgwQJmxwYQWEzAbxqG4Qr1IQCRH8a+/fZbeeqpp2TXrl3y2Wef6csdO3aE+rDCHPOMAQAQ7mIkglRWVsr58+f1dYfDIZ06dQr1IQEwIQtzU5kGlZuIqpqxrVu3ysSJE6Vr1676h2zjxo11tlm9erX06NFD4uPjZdCgQbJt2zaf99+xY0e5//77JS0tTT/HDTfcID179vTX4ZsTf1AAAIiemrHS0lIZMGCAzJ49W2699dY6969fv14WLlyoA9mIESPkmWeekQkTJsj+/ft1wFJUQCsvL6/z2M2bN0urVq3kjTfekK+//lpfV49VAfC6666r93jUfqrvq7i42FujppZA8Ow3UPtvLqvLJTYRcbqc4gqTY4oE4VaOCH45VjoqvZO+8jkwx3fS5XRRliEWjb+tDh9fq8Uw/D8JlaoZ27Bhg0yZMsW7btiwYTJw4EBZs2aNd13v3r31NitWrGhyn6+++qq8++678vTTT+vbjz/+uO7A/8ADD9S7/cMPPyzLli2rs/7ll1+WhIQEiQZ9jq6XywvelEMdx8u+7neG+nCAiFHoLJRVZ1dJnMTJz9v9PNSHgxbouPwxaV96Wv72g3ulZ79uvJcIqrKyMrnjjjvkzJkzkpSUFNo+YxUVFbrD/eLFi2usHzdunGzfvt2nfaSmpuptVZ8xu92ug9m//du/Nbj9kiVLZNGiRTVqxtQ+1HM29oa0NAHn5OTI2LFj9TGGmvXtnSIFopuG08dmhfpwIka4lSOCX455xXmy6o1V+nFZWXx3Irksd/3iV/qyV69eMj7ruwE6QvgiGn9bi6ta5ZoSlDBWWFgoTqdTUlJSaqxXt48fP+7TPq655hr9o3j11VeL1WqV66+/XiZNmtTg9nFxcXqpTX0AAv0hCMZz+MSmGilFbFar2MLheCJM2JQjgl6O1bfnM2CO76TVZqUsw0Q0/bbafXydMaEcoaSaGZszaumRRx7RCwAAzcJp4RDt84x16NBBbDZbnVqwgoKCOrVl8CfmGQMAINwFJYzFxsbqkZKqrbg6dXv48OHBOAQAQDRiih9EAL81U5aUlMihQ4e8t3Nzc2XPnj2SnJysp65QnelnzJghgwcPlszMTFm7dq3k5eXJvHnz/HUIqI1zUwItoqa2AICICWM7d+6U0aNHe297RjLOnDlT1q1bJ9OnT5eioiJZvny55OfnS0ZGhmRnZ0t6erq/DgEA/MLiaeIHgEgKY6NGjWryxN3z58/XC4KFPmMAAIS7iDlROAAAgBkRxsyMPmMAAIQ9whgANCAAZ4tDiBgu3nqEL8KYqdFnDACAcEcYA4DaGExpIhQmwh9hzMzoMwYAQNgjjEUD+r0AABC2CGOmRvU80BLMwA8gGAhjUYERYQAAhCvCmJlxglzg4r461CoDCCLCWDSgzxiAqEcLAcIXYczU6DMGAEC4I4yZGVNbAIhyBv8nRQQgjAEAAIQQYczUOB0ScFHfHAa/AAgiwhgAAEAIEcbMzNtXglFEAACEK8IYADTAYFoYAEFAGDM1+owBABDuCGMAUAsz8JuP4XKF+hCAyApjU6dOlfbt28u0adPq3PfGG2/IFVdcIZdffrk899xzITm+iME8YwCiHhONIfyFZRhbsGCBvPjii3XWV1ZWyqJFi+Ttt9+W3bt3yy9/+Us5efJkSI4RAADAtGFs9OjRkpiYWGf9Rx99JH379pVu3brp+7OysmTTpk0hOcbI6jMW6uMAAAB+C2Nbt26ViRMnSteuXfXEiBs3bqyzzerVq6VHjx4SHx8vgwYNkm3btok/HDt2TAcxj+7du8vRo0f9sm8AqM3gfzIAwjGMlZaWyoABA+Spp56q9/7169fLwoULZenSpfLJJ5/IyJEjZcKECZKXl+fdRgW0jIyMOosKW80dZs5M2Y2gzxhwUejADyCYYpr7ABWs1NKQlStXypw5c2Tu3Ln69qpVq3RT4po1a2TFihV63a5duy7qYFWtWPWasCNHjsiwYcPq3ba8vFwvHsXFxfrS4XDoJRA8+w3U/pvL6nSJTURcLpc4w+SYIkG4lSOCX46OyguP4XNgju+k+h2kLEMrGn9bHT6+1maHscZUVFTooLV48eIa68eNGyfbt29v8f6HDh0qe/fu1YEsKSlJsrOz5cEHH6x3WxX8li1bVmf95s2bJSEhQQIpJydHwsF3TnwhfXVoPSyfZGeH+nAiTriUI4Jfjqddp/Wl0+nUvzOI3LK8xOnUl4cOHZLs7Av/QUfoRNNva1lZWfDDWGFhof7xSklJqbFe3T5+/LjP+xk/frweLamaRFW/sA0bNsiQIUMkJiZGnnzySd3BX/0v54EHHpBLLrmk3n0sWbJEj7ysXjOWmpqqg6EKcoFKwOpDNnbsWLHb7RJq1g8OiRxz963rkpUV6sOJGOFWjgh+OeaX5ssTrz8hNptNDxRC5Jblzkef1Jc9e/aUrKwbAnSE8EU0/rYWV7XKBTWMNdSPS/X1ak7frsZGSE6aNEkvTYmLi9NLbeoDEOgPQTCewyc2d/FaLRaxhsPxRJiwKUcEvRztMXbvbxefgQj/Tlb97bHabJRlmIim31a7j6/Tr1NbdOjQQf9PsnYtWEFBQZ3aMgAAAPg5jMXGxuqRkrXbg9Xt4cOH834HHeemBC7qm9OMmnwAaKlmN1OWlJTojpAeubm5smfPHklOTpa0tDTdT2vGjBkyePBgyczMlLVr1+ppLebNm9figwUAAJBoD2M7d+7UHeg9PJ3kZ86cKevWrZPp06dLUVGRLF++XPLz8/X8YWo0Unp6un+PHE1jnjEAAMwXxkaNGlXv5KvVzZ8/Xy8AEMmYgR9A1J6bEv5CnzEAAMIdYQwAYHoWV+MtOkAoEcbMjD5jAKIcEQyRgDAGAAAQQoQxU6PPGNASdOAHEAyEMQAAgBAijJkZfcaAi/vqeGqVASAICGMAAAAhRBgzNfqMAQAQ7ghjAADTMwxXqA8BaBBhzMzoMwa0DJNUmeh3EAhfhDEAqMXCH3AAQUQYMzX6jAEAEO4IYwAAACFEGDMz+owBABD2CGMA0ABOhwQgGGKC8iwIrbMnRL56VySmlUhMnEhMvIg93n3pXeIYdQRUYQZ+AMFEGDMza1XxHt4h8uLkprf3hLLGQpv3dtV2+nYLLq1UzgIIPINpShDGCGNmdsUEkYM5ImfzRSrPuxdH1WVluUjlOZHqEyF6tpEzwTtGW2y1cKbCXisfLy8y/Nns1AACUcTgPKOIAIQxM0vsLPL9lxv/r6KrUsRx7kI405fVQ9v5Rm6fa+aletw5EZfjwjE4K9xLeZACoMXaSFhzhz2bLU4GnigS25s5InGtmxkSa12qhdo/AEAjCGPRPtpS1RSpJZhczqoAeN6Pl02EQQ9VE+godS8NUA2nqerKqe3+eb22uGbW4PkY9vT1qkt7woX7CH9+Qwd+AFEbxqZOnSrvvvuuXH/99fLnP//Zu/7w4cMyY8YMKSgokJiYGPn5z38u3/ve90J6rLgIVptIXBv3EgyqBtBT89dQWFP3VwU75/lS+fyzT6T35ZeKzVXhh9q/cvcSrObf2gFNBTzvdc9l9esJVQGvgfu862qFPhPPUs8M/AAk2sPYggUL5Ic//KG88MILNdarALZq1Sq56qqrdCAbOHCgZGVlSevWrUN2rIgAKjToQBIv0qrpzV0Oh3xZkC1XXJslNvtF1ho6K6s16Qaw9s9R5r5fh70qOlyeEzl3UgKqdmBTAS22dbV1CSKxVZeebbz3V13WuN+zfdX99O8DECXCMoyNHj1a14zV1qVLF70onTp1kuTkZDl58iRhDOHHFiNiC2Ltn6fpVy9lFy51iCur/z7vusbuq7UP1b/PQ68rE5GiwLwmi61uQKsd4PT11rUu1frWYrHGScfivWI50kGkVVLdx6gyAoAw0Oxfo61bt8rjjz8uu3btkvz8fNmwYYNMmTKlxjarV6/W26j7+/btq2uzRo4c6c/jlp07d4rL5ZLUVN27B4huwWr61TV+9YU3T2ArE6mouvTcX1FabbsG1unHqNul7kEliuEUqTjrXi7yx224uvLlrxruy6eCWWwbb4Bz19ypsBZbdQwukb8vc6+PS3Rvq95jfem5nVi1LpGAByA4Yay0tFQGDBggs2fPlltvvbXO/evXr5eFCxfqQDZixAh55plnZMKECbJ//35JS0vT2wwaNEjKy6s1q1TZvHmzdO3atcljKCoqkrvuukuee+65BrdR+6/+HMXFxfrS4XDoJRA8+w3U/hEclGMTrPEicWppH5gCcDqqBTd3SLNUD3EqDFaUVa27EP4semDGhdtGRamUnCyQxHirWKoFP4tnOhfVtHtOLafqHoPNKpLWXQzV3/C9lT4fuqGbai+ENUMHvAsBzqgW4tzXq7ZVtz3hzrNe9f0zcb+84H0n3ROMOZ1OfptDLBp/Wx0+vlaLoX9tLr6Ta+2asWHDhum+XGvWrPGu6927t95mxYoVPu9bNVM+9dRTNTrwKypgjR07Vu6++27dmb8hDz/8sCxbtqzO+pdfflkSEhJ8Pg4AJmIYYjUcEuMqF5urXGKcVZeu81WX7tslzrMyP3aHWAyRDaVXSozzvN7GfXmu2u1z7scaVbV5fuQSqzht8eKwxkulrZVU6sv4apetmlzv8K5XwS46J1hu/8gT0rG4ULKnz5PvDLw01IeDKFNWViZ33HGHnDlzRpKSkhrczq+dJioqKnTz5eLFi2usHzdunGzf3vJpAlRunDVrlowZM6bRIKYsWbJEFi1aVKNmTDVpqmNp7A1paQLOycnRYdF+sR2/EXKUozm0pByLzhWJbBira6bS/u3lJkKT6rKn5sorEalwL5Zy1bxadbu8RCze6+71+rZn+/KzF+7X97mnXbGqOOYsE7tT1ehJixmqpk3Xvrlr64xGauV0rV0jtXjBHk3bkrL86Jer9OVll/WUrKzxATpC+CIaf1uLq1rlmuLXMFZYWKirglNSUmqsV7ePHz/u837Gjx8vu3fv1k2i3bt317VvQ4YMkffff183g/bv3182btyot33ppZekX79+dfYRFxenl9rUByDQH4JgPAcCj3KM3nKMqbzw0+jTY9U28WpUd83fvoviqpoLTwU3Hdg8l1UBrrz4wnXvpdrmbP3rqvrgWTyjbKum2LO09FRrNfrNVQtvTa5LFIlvK9KqvfuyGQMpWvKdtFmt/C6HiWj6bbX7+DpjgjFHj6rRas68PZs2bap3/bXXXqs77QOAaalJez0Bxl9z7FWrlbsQ2IrrCW+1t6se8M5WjZ5VgbFS5Pxp99JScUki8e1EWqmlfbVLFdbc1y2xSdLh7AGRE2kiiR3d61UNH33qYBJ+DWMdOnQQm81WpxZMzQlWu7YMAMJdxM/AX32OvdYd/DOFSu1auTo1dZ4w11DN3tmqIFjVfOO5fiav0T9UI9SVQ49dWGm11xvcage6OEOdb1ck4Vy+SGmhezumNYGZw1hsbKweKanahNUs+h7q9uTJk/35VACAUEyhopoW1eKPUbPnz4icO+0e0aqW89Wue9afPy2uspNS+u1haRPjFItap85woZbSb91LI9pXdpIKiZHh/1wscqyqH7FqKvWGtiYCXfV11MYhXMJYSUmJHDp0yHs7NzdX9uzZoydgVVNXqE7zqnP94MGDJTMzU9auXSt5eXkyb948fx87AASEpWU9quALdYYFVVvnQ42d0+GQt7Oz9RlX7DEx7ilOvMGt8TDnsKi/V4Y41IhSqZq02DN/XSO1cQ32lasd3BKS3a8hQb2WjtWuVy0qwAH+DmNqslU1Q76HZ8TizJkzZd26dTJ9+nQ9D9jy5cv1pK8ZGRmSnZ0t6enpzX0qAADqNr16Jjhu273Jd6fomdGSLMfl3cFrZPKd46tq4xquhauzzrPo2rhKn2rjalCjWD0hrU0n93V12SblwvXW6nYnd40j/eCiUrPD2KhRo9wTITZi/vz5egEAILSq1XKqvmKtL3EvzaH+5qnBC7WDW9lJ9zlgVV80vXwrUqYui9zX1cTCagSrqoHzpRbOFusOaWpJ7FzzemKXC5cJl7gHesA0ODkbAACNUbVVntNl+VAb5w1watCCrkmrCmolBdUuC9yXnnVqEIOar+7MYffSVHOpCmXt0txL29QL19WS1E0kpuqUXogIhDEAAALSnFo1RUnyZU1vr07ZpULZ2RMiJcdFzh4XKTlR7TLffV1to5pLPaHtm/fre3KRpK4NhzUVKGPqzsOJ0CGMAUAtzZkXEfALe6sLYakxlRXuWrUzR0ROq0CWJ3Lasxx2X6rm0eKj7iXvg3p2YnE3edYJaup6ujusqeNB0BDGAACIFKr5UYUltaRdU/d+NTG6qj1TtWanv6kb1NSi+rHpmrZ8kcMf1v88qq+aJ6hd8h2RTleKdOztvk4TqN8RxgAAMAvVsT9RdfpPEek+uP6+bGVF1YJatZDmWdTpuFTTqFqO7qy1/xiR5J4Xwpm+vNId0tR0JbgohDEAAKKFaoL3zIHWbVD9YU2NFtVhrap27dsvRL49IFJwwD0/W+EX7kVerxnSVCBTwayTCml9RLoMcNes0ezfJMIYADSiuefWRZhqYkomVFGfdTWRrVq6Xl33PVT90FQoU+Hs288vXFcjR/W6AyL7N154jJoYV4W+boPF0nmAxFae5a2uB2EMAGphBn4TIUj797309Fe7/IaaIU0NKNC1Z5+7L49/5r6uatkO/V0vKnBMUJsffVIkfbhI2nD3ZftLo76cCGMAAKBlIU2PxEwVuXzshfWV5SIn9ooc3S1ydJcYRz4WS9EhsZz8SkQtn/zBvV1iV5FLrxXpOVrkOze4z0YQZQhjAAAgAAkjrqqJUvVNu1sqHQ7J+eurMq5PO4k58qHIN9tFjn0icvaYyGf/614U1Tz6nbEivW4U6TYwKmrNCGMAACAoHDGtxVBBq3eWe0VFmciRj0Vyt7ibM/P/6Q5oatn6K5Gk7iJ9p4j0v02kc3/TBjPCGAA0whCDPmRAoMQmiFz2Xfdy/YPuMxCoUHZws/uy+IjIB0+5FzVCs/90dzBTZxgwEcIYANRCB34gRBJTRK6+0704zot8+Q+Rz14VOZAtUrBf5O8PifxjuUjviSKZPxZJHWqKoiKMAQCA8GOPF7nyJvdy7rR7yow9r4gc3uG+rpa0TJHr/kOk55iIbsK0hvoAAAAIxnxxiGCt2okMmiUyZ5PIvPdFrv6BiNXuPvfmH24RWXeTyJFdEqkIYwAA0zIit7IEDemcITL5aZGFn4pcM1/EFifyzfsiz40R+b/73LVoEYYwBgCNoEYFCFNJXUVuXCGy4BORAd93r9u1TmR1pkjuVokkhDEAABC52nYTmfpbkVlvuk9iruYte2GSyAdPR8xpsAhjAFAL56IEItCl14rM2yZy1Q/0pDSy6Wcif3tAxOWUcEcYAwAA5hDbWmTyUyJj/9t9+6O1Iq/NCftAFpZhbOrUqdK+fXuZNm1avfeXlZVJenq63H///UE/NgAAEMYsFpERC0S+94KILVZk3waR7P8I6ybLsAxjCxYskBdffLHB+x955BEZNmxYUI8JAABEkL5TRG79nZ7GWXb+zt25P0yFZRgbPXq0JCYm1nvfwYMH5cCBA5KVVXVeKwAI8OmQYAJhXCuCAOozyX2aJUX1ITudJ6YIY1u3bpWJEydK165ddSfXjRs31tlm9erV0qNHD4mPj5dBgwbJtm3b/HW8umlyxYoVftsfAMDMmGgs6o1YKJI2XMRRFrbNlc0OY6WlpTJgwAB56qmn6r1//fr1snDhQlm6dKl88sknMnLkSJkwYYLk5V1IoyqgZWRk1FmOHTvW6HO//vrr0qtXL70AAAA0yWoVufnX7hn7//WW+3yXkX5uShWs1NKQlStXypw5c2Tu3Ln69qpVq2TTpk2yZs0ab43Wrl0Xd8qCHTt2yJ/+9Cd59dVXpaSkRBwOhyQlJcmDD1ZVQVZTXl6uF4/i4mJ9qR6jlkDw7DdQ+0dwUI7m0JJyrKysrLEfwxp+/5OOJv74TjpdLn6bo/m3tX1PsQ6aLbaP14rr49+LM/27QXlaX1+rX08UXlFRoYPW4sWLa6wfN26cbN++vcX7V2HOE+jWrVsne/furTeIebZdtmxZnfWbN2+WhIQECaScnJyA7h/BQTlGbzmec53zXv/b3/4mNovNz0eFYJVl26pg/XVurmRnZ/PGR/Fva+K5S2WMuvLFW/KP11+RcnvbgD+nmv0h6GGssLBQnE6npKSk1Fivbh8/ftzn/YwfP152796tm0S7d+8uGzZskCFDhjTrWJYsWSKLFi2qUTOWmpqqg6GqTQtUAlYfsrFjx4rdbg/IcyDwKEdzaEk5FlcUyyN/fkRfv3HCjWJXzRuIyLL88PH/0ZeX9ujBwK8QC4ffVtfzfxbrsd0yNqVIXNdUnUIpgDytckENYw3NXq3O7dacGa1Vs2ZTZs2a1ej9cXFxeqlNfQAC/SEIxnMg8CjH6C1Hu2Gv+XjCWMR/J21WK7/LYSKkv60D7xI5tlts/3xFbNcudM9JFkC+vk6/Tm3RoUMHsdlsdWrBCgoK6tSWAQAABFXGrSL2BJHCL0RO7JVw4dcwFhsbq0dK1m4PVreHDx/uz6cCAMB3YTidAUIgPkkkLdN9/ZsPwqYImt1MqUYxHjp0yHs7NzdX9uzZI8nJyZKWlqb7ac2YMUMGDx4smZmZsnbtWj2txbx58/x97AAANC7AzVCIQGnXuKe3OLxDZNi/SUSGsZ07d+oZ8j08neRnzpypRzhOnz5dioqKZPny5ZKfn6/nD1MjWNS5JAEg4lChAphLatXpFA9/LOGi2WFs1KhRukN+Y+bPn68XAIhEFmZtB8wrpa/78sxhkcoKkZjYUB9ReJ6bEgAAICASLhGxqQBmiJT4Pu1WIBHGAABAdPUjTOrqvl7c+GkYg4UwBgAAoktSN/flmSMSDghjANAIgx78gHnDWDE1YwAQlujAbz5NDTxDlEmimRIAACAMasaOhkUp0EwJAACiSxI1YwAAAKGT1MV9eTY/LEqBmjEAABBdYhPdlxWlEg4IYwDQCEZTAiYUE+e+rCyXcEAYA4BaLJxcGjC3mHj3ZeV5NdQ21EdDGAMAAFFaMyaGiNMR4oMhjAEAokEY1H4gjNhbXbheeU5CjWZKAIBpGTQ5oz76ROESNv3GCGMA0AhmbgdMyGKp2W8sxAhjAFALp0MCoqjfmIMwBgAAELqmSmeFhBo1YwAAIPpYPBEo9IM7CGMAACB6w5jhCvWREMYAAEAUshDGAAAIGkbFog7CWOOmTp0q7du3l2nTptW5Lzc3V0aPHi19+vSRfv36SWlpeJzkEwAQhphnDE19NsJgQuCw7DO2YMECefHFF+u9b9asWbJ8+XLZv3+/bNmyReLiPKc0AAAA8JHF5r6kz1j9VM1XYmJinfX79u0Tu90uI0eO1LeTk5MlJibG17cdAAAg8pspt27dKhMnTpSuXbuKxWKRjRs31tlm9erV0qNHD4mPj5dBgwbJtm3b/HKwBw8elDZt2sikSZNk4MCB8uijj/plvwAAIMpYwieMNbtaSfXRGjBggMyePVtuvfXWOvevX79eFi5cqAPZiBEj5JlnnpEJEyboZsW0tDS9jQpo5eV1zwW1efNmHfIa4nA4dLDbs2ePdOrUSW688UYZMmSIjB07ts62av/Vn6O4uNi7D7UEgme/gdo/goNyNIeWlGP1x6jrNqOqOQOR952s6g7kchn8NodYuP22xljc59qodFSIEeBc0OSxNHfHKlippSErV66UOXPmyNy5c/XtVatWyaZNm2TNmjWyYsUKvW7Xrl1yMbp3767DV2pqqr6dlZWlg1l9YUw917Jly+oNfAkJCRJIOTk5Ad0/goNyjN5yLDcu/EdO/X7ZLXY/HxWCVZaJle4/ht9887VkZ2fzxoeBcPltHV1SKkki8uGOD6Rw35mAPEdZWZlP2/m1w1VFRYUOWosXL66xfty4cbJ9+/YW718FsRMnTsipU6ekbdu2usn0Rz/6Ub3bLlmyRBYtWlSjZkyFOHUsSUnq7Q9MAlYfMhUOVd82RCbK0RxaUo5ljjL571f/W18fP368xHtOKIyIK8sdT67Wl+npl+r/wCN0wu23NeboYyLnj8qwoUPF6HFdQJ7D0yrX5LH480kLCwvF6XRKSkpKjfXq9vHjx33ej/rx2717t24SVbVhGzZs0EFMddZX/cSuu+46PWeMClY333xzvftQoyzrG2mpPgCB/hAE4zkQeJRj9JajXew1Hx/D9zliv5NVsxdYLe7HI/TC5rfV4u5+EGPTH46APIWvrzMgQxFVx/7qVHCqva4xqlngYptJAQCo9heJNwNNzDPmMtc8Yx06dBCbzVanFqygoKBObRkAAEDIR1O6TBbGYmNj9UjJ2p3z1O3hw4f786kAAABMcXaGZjdTlpSUyKFDh2qcnkiNaFQTsKqpK1Sn+RkzZsjgwYMlMzNT1q5dK3l5eTJv3jx/HzsAAEDEa3YY27lzp54h38MzYnHmzJmybt06mT59uhQVFelTFuXn50tGRoYeTpyenu7fIwcAAIjGMDZq1CjdIb8x8+fP1wsAAAAi8EThAAAA0YIwBgAwvSYadICQIowBAEzLCJ8Bc0CDCGMAAAAhRBgDAAAIIcIYAABACBHGAAAAQogwBgAAEEKEMQAAgEiagT9Sec4aUFxcHLDncDgcUlZWpp/DbrcH7HkQWJSjObSkHMscZeI859TX1eMrYioCdJQIdFmWVlZKK6dTyspKA/r7jwj8bT1XKVJuiJSUqi96QJ7C85lr6sxFFqOpLUziyJEjkpqaGurDAAAAUebw4cPSvXv3Bu+PmjDmcrnk2LFjkpiYKBaLJWAJWAU+9aYnJSUF5DkQeJSjOVCO5kFZmkM0lqNhGHL27Fnp2rWrWK0N9wyLmmZK9SY0lkr9SX3IouWDZmaUozlQjuZBWZpDtJVj27Ztm9yGDvwAAAAhRBgDAAAIIcKYH8XFxclDDz2kLxG5KEdzoBzNg7I0B8qxYVHTgR8AACAcUTMGAAAQQoQxAACAECKMAQAAhBBhDAAAIIQIY360evVq6dGjh8THx8ugQYNk27Zt/tw9muHhhx/WZ1qovnTu3Nl7vxq3orZRsyK3atVKRo0aJfv27auxj/Lycrn33nulQ4cO0rp1a5k0aZI+rVZ1p06dkhkzZuhJ/dSirp8+fZqyukhbt26ViRMn6nJRZbZx48Ya9wez3PLy8vSxqH2ofS1YsEAqKjhHpT/KcdasWXW+n9dccw3lGGZWrFghQ4YM0Weu6dSpk0yZMkW++OKLGtvwnfQTNZoSLfenP/3JsNvtxrPPPmvs37/fuO+++4zWrVsb33zzDW9vCDz00ENG3759jfz8fO9SUFDgvf+xxx4zEhMTjddee8347LPPjOnTpxtdunQxiouLvdvMmzfP6Natm5GTk2Ps3r3bGD16tDFgwACjsrLSu82NN95oZGRkGNu3b9eLun7zzTcH/fWaRXZ2trF06VJdLurnacOGDTXuD1a5qW3VOvVYtQ+1r65duxr33HNPkN4Jc5fjzJkzdRlU/34WFRXV2IZyDL3x48cbzz//vLF3715jz549xk033WSkpaUZJSUl3m34TvoHYcxPhg4dqn88qrvyyiuNxYsX++sp0Mwwpv4A18flchmdO3fWPyIe58+fN9q2bWv89re/1bdPnz6tw7UK2R5Hjx41rFar8dZbb+nbKnSrPzQ7duzwbvPBBx/odQcOHKC8Wqj2H/FglpsKE+ox6rEer7zyihEXF2ecOXOGsm1BOXrC2OTJkxt8DOUYntR/aFV5btmyRd/mO+k/NFP6gWq62LVrl4wbN67GenV7+/bt/ngKXISDBw/qZhLVdHz77bfLV199pdfn5ubK8ePHa5SXmozwu9/9rre8VHk6HI4a26h9ZWRkeLf54IMPdBPXsGHDvNuopha1jnL3v2CWm9pGPUY91mP8+PG6CVQ9B1ru3Xff1U1fvXr1krvvvlsKCgq891GO4enMmTP6Mjk5WV/ynfQfwpgfFBYWitPplJSUlBrr1W31xwPBp/7Qvvjii7Jp0yZ59tlndTkMHz5cioqKvGXSWHmpy9jYWGnfvn2j26g/JrWpdZS7/wWz3NRl7edR+1T7pmxbbsKECfLHP/5R3n77bXnyySfl448/ljFjxuiwSzmGJ1XJuWjRIrn22mv1f1QUvpP+E+PHfUU91Qm19oe39joE78feo1+/fpKZmSk9e/aUF154wdtR+GLKq/Y29W1PuQdWsMqNsg2c6dOne6+rP+yDBw+W9PR0efPNN+WWW25p8HGUY+jcc8898umnn8p7771X5z6+ky1HzZgfqJFWNputzv+YVbV77f9dIzTUiDgVylTTpWdUZWPlpbZRzc9q1F1j25w4caLOc3377beUewAEs9zUNrWfR+1TNYHynfa/Ll266DCmvp+UY/hRo5P/+te/yjvvvCPdu3f3ruc76T+EMT9QTRdqKoucnJwa69Vt1TSG0FPNH59//rn+0Vd9yNSPSPXyUn/At2zZ4i0vVZ52u73GNvn5+bJ3717vNqq2TfWh+Oijj7zbfPjhh3od5e5/wSw3tY16jHqsx+bNm3UfNfUc8C/VfeDw4cP6+0k5hg9VE6lqxP7yl7/oJmX1HayO76Qf+XEwQFTzTG3xu9/9To/WWrhwoZ7a4uuvvw71oUWln/70p8a7775rfPXVV3rUnJq2QE2J4CkPNSJPjcL7y1/+oqdI+P73v1/vFAndu3c3/v73v+vpDcaMGVPvFAn9+/fXo/HU0q9fP6a2aIGzZ88an3zyiV7Uz9PKlSv1dc8UMcEqN8/UFtdff73eh9qX2idTW7S8HNV96vupphTJzc013nnnHSMzM1NPR0I5hpd///d/19839VtafRqSsrIy7zZ8J/2DMOZHTz/9tJGenm7ExsYaAwcO9A7/RfB55p9SAVnND3XLLbcY+/bt896vhmSr6S/UVAlquoLrrrtO/3Gv7ty5c/qPb3JystGqVSv9xzovL6/GNmpupDvvvFMHPbWo66dOnQra6zQb9YdZ/fGuvaipEIJdbio4qHmV1D7UvtQ+1VQaaFk5qj/k48aNMzp27Ki/n2reKrW+dhlRjqFXXxmqRc095sF30j8s6h9/1rQBAADAd/QZAwAACCHCGAAAQAgRxgAAAEKIMAYAABBChDEAAIAQIowBAACEEGEMAAAghAhjAAAAIUQYAwAACCHCGICoc/z4cbn33nvlsssu0yf/Tk1NlYkTJ8o//vGPFu973bp10q5dO78cJ4DoEBPqAwCAYPr6669lxIgROjD96le/kv79+4vD4ZBNmzbJj3/8Yzlw4AAFAiCoqBkDEFXmz58vFotFPvroI5k2bZr06tVL+vbtK4sWLZIdO3bobfLy8mTy5MnSpk0bSUpKkttuu01OnDjh3cc///lPGT16tCQmJur7Bw0aJDt37pR3331XZs+eLWfOnNHPoZaHH344hK8WQCQgjAGIGidPnpS33npL14C1bt26zv2qtswwDJkyZYredsuWLZKTkyNffvmlTJ8+3bvdnXfeKd27d5ePP/5Ydu3aJYsXLxa73S7Dhw+XVatW6YCWn5+vl/vvvz/IrxJApKGZEkDUOHTokA5bV155ZYPb/P3vf5dPP/1UcnNzdV8y5aWXXtK1Zyp8DRkyRNec/cd//Id3P5dffrn38W3bttU1Yp07dw7CKwJgBtSMAYgaKogpKiw15PPPP9chzBPElD59+uhaM3Wfopo0586dKzfccIM89thjuuYMAC4WYQxA1FA1WCqIeUJVQ4GtvrBWfb3qB7Zv3z656aab5O2339ZhbcOGDQE9dgDmRRgDEDWSk5Nl/Pjx8vTTT0tpaWmd+0+fPq2DlWqGPHz4sHf9/v37daf83r17e9epjv8/+clPZPPmzXLLLbfI888/r9fHxsaK0+kM0isCYAaEMQBRZfXq1TosDR06VF577TU5ePCgrin7zW9+I5mZmbrpUU13oTrp7969W4+6vOuuu+S73/2uDB48WM6dOyf33HOPHjn5zTffyPvvv6/7knmC2qWXXiolJSV6zrLCwkIpKysL9UsGEOYIYwCiSo8ePXTIUlNT/PSnP5WMjAwZO3asDk9r1qzRTZEbN26U9u3by3XXXafDmZocdv369frxNptNioqKdEBTtWNq2osJEybIsmXL9P1qROW8efP06MuOHTvqucwAoDEWw9OjFQAAAEFHzRgAAEAIEcYAAABCiDAGAAAQQoQxAACAECKMAQAAhBBhDAAAIIQIYwAAACFEGAMAAAghwhgAAEAIEcYAAABCiDAGAAAgofP/AWc0Hchag/FoAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mu_list = [10,20,50,100]\n",
    "plt.figure(figsize=(7,4))\n",
    "for mu in mu_list:\n",
    "    f_t=non_lin_cst(func.Rosen(),square2(),mu)\n",
    "    costs,x=central_path(f_t,10,1.e-9,np.array([0,0]))\n",
    "    cost_min=np.min(costs)\n",
    "    plt.plot(costs - cost_min)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Cost')\n",
    "plt.title('Evolution of the cost')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez tracer l'évolution des coûts en échelle log et exhiber la convergence linéaire de ces algorithmes\n",
    "<div class=\"alert alert-block alert-info\"> METTEZ ICI VOTRE ARGUMENT POUR JUSTIFIER DE LA CONVERGENCE LINEAIRE DE CET ALGORITHME\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1306200897.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mLogique car on multiplie chaque fois par 10 le coût (t)\u001b[39m\n            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Logique car on multiplie chaque fois par 10 le coût (t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant s'intéresser à l'influence de $\\mu$. On fixe maintenant `varepsilon` à $10^{-6}$ et on va tracer la courbe d'évolution de la fonction à minimiser (en échelle log, et en traçant la fonction auquelle on a retiré `costmin`) pour différentes valeurs de $\\mu$ dans `mu_list` ci-dessous. La valeur de `cost_min` est inchangée et est la valeur trouvée pour `varepsilon` à $10^{-9}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_list=[2,4,10,100,1000,5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyez-vous une grande différence quand $\\mu$ varie ?\n",
    "<div class=\"alert alert-block alert-info\"> METTEZ ICI VOTRE ARGUMENT \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problème avec multiples contraintes linéaires\n",
    "On s'intéresse maintenant aux problèmes de la forme\n",
    "$$\n",
    "\t(\\mathcal{P}_{t, \\text{ lin.}}) \\qquad \\umin{x\\in \\mathbb{R}^d, } f_t(x) \\eqdef f(x) - \\frac{1}{t} \\text{Log}( b-A x  ),\n",
    "$$\n",
    "qui approximent, quand $t$ tend vers $+\\infty$ des problèmes du genre\n",
    "$$\n",
    "    (\\mathcal{P}_{\\infty,\\text{ lin.}}) \\qquad \\umin{x\\in \\mathbb{R}^n, A x \\leq b} f(x)\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> Calculez ci-dessous le gradient et la Hessienne de $f_t$ en fonction de $A$,$b$,t et du gradient et de la Hessienne de $f$  \n",
    "$$\\nabla f_t(x) = ?? \\quad H[f_t](x) = ?? $$\n",
    "</div>\n",
    "\n",
    "Créez une classe de fonction `class lin_cst()` qui prend au constructeur la fonction `f`, la matrice `A`, le vecteur `b` le scalaire `t` et  et qui calcule la valeur de $f_t$. Cette fonction devra agir comme `class non_lin_cst()` et notamment avoir un attribut `nb_constraints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lin_cst() :\n",
    "    def __init__(self,f,A,b,t) :\n",
    "        self.zeros()\n",
    "        self.f=f\n",
    "        self.A=A\n",
    "        self.b=b\n",
    "        self.t=t\n",
    "        self.n = b.shape[0]\n",
    "        self.nb_constraints=1.\n",
    "    def zeros(self) :\n",
    "        self.nb_eval=0\n",
    "        self.nb_grad=0\n",
    "        self.nb_hess=0\n",
    "    def value(self,x) :\n",
    "        self.nb_eval += 1\n",
    "        g=A@x - b\n",
    "        l=0*g\n",
    "        l[g>=0]=np.inf\n",
    "        l[g<0]=-np.log(-g[g<0])\n",
    "        return self.f.value(x) + l.sum()/self.t    \n",
    "    def grad(self,x) :\n",
    "        self.nb_grad += 1\n",
    "        tmp=1./self.t*self.A.T@(1./(self.b-self.A@x))\n",
    "        return self.f.grad(x) + tmp   \n",
    "        \n",
    "    def Hess(self,x) :\n",
    "        self.nb_hess += 1\n",
    "        tmp = 1./self.t*self.A.T*(1./(self.b-self.A@x)**2)@self.A\n",
    "        return self.f.Hess(x) + tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = np.array([1,2])\n",
    "u.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va tester notre fonction dans la case ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.717787999246487 inf\n",
      "[-9.36529883 -1.43185282] [-624.84901174 -313.83869195]\n",
      "[[1624.61765301  896.38712898]\n",
      " [ 896.38712898  871.39523753]]\n",
      "[[2531.87148488  934.54404056]\n",
      " [ 934.54404056  476.99362034]]\n",
      "## TEST DE DERIVEE NUMERIQUE##\n",
      "eps 1.0e-01 grad inf ratio 8.1e-01 angle 2.0e+00\n",
      "eps 1.0e-02 grad 8.3e-01 ratio 1.9e+00 angle 4.8e-04\n",
      "eps 1.0e-03 grad 4.9e-02 ratio 6.8e-02 angle 4.7e-06\n",
      "eps 1.0e-04 grad 4.7e-03 ratio 6.4e-03 angle 4.7e-08\n",
      "eps 1.0e-05 grad 4.7e-04 ratio 6.3e-04 angle 4.7e-10\n",
      "eps 1.0e-06 grad 4.7e-05 ratio 6.3e-05 angle 4.7e-12\n",
      "eps 1.0e-07 grad 4.7e-06 ratio 6.3e-06 angle 4.7e-14\n",
      "eps 1.0e-08 grad 4.8e-07 ratio 6.3e-07 angle 3.3e-16\n",
      "eps 1.0e-09 grad 6.0e-08 ratio 6.5e-08 angle 2.2e-16\n",
      "eps 1.0e-10 grad 8.4e-07 ratio 3.6e-08 angle 1.1e-16\n",
      "eps 1.0e-11 grad 1.5e-05 ratio 2.8e-07 angle 1.0e-13\n",
      "eps 1.0e-12 grad 7.7e-05 ratio 8.7e-06 angle 1.0e-15\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "x=np.zeros(2)\n",
    "n=10\n",
    "A=np.random.randn(42,2)\n",
    "b=np.abs(np.random.randn(42))\n",
    "x2=np.random.randn(2)\n",
    "f_t=lin_cst(func.Rosen(),A,b,10)\n",
    "print(f_t.value(x),f_t.value(x2)) ## 4.717787999246487 inf\n",
    "print(f_t.grad(x),f_t.grad(x2)) ## [-9.36529883 -1.43185282] [-624.84901174 -313.83869195]\n",
    "print(f_t.Hess(x)) ## [[1624.61765301  896.38712898] [ 896.38712898  871.39523753]]\n",
    "print(f_t.Hess(x2)) ## [[2531.87148488  934.54404056][ 934.54404056  476.99362034]]\n",
    "\n",
    "print('## TEST DE DERIVEE NUMERIQUE##')\n",
    "d=np.random.randn(2)\n",
    "opt.deriv_num(f_t,5.e-3*x2,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'intéresse maintenant au problème particulier du [Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics)), pour retrouver un signal parcimonieux. Le problème du Lasso s'écrit $$\n",
    "        \\umin{w \\in \\mathbb{R}^p} \\frac{1}{2}\\Vert Bw-y \\Vert_2^2 + \\lambda \\Vert x \\Vert_1\n",
    "$$\n",
    "On suppose ici qu'on veut retrouver un signal $w_0$ que l'on sait être parcimonieux (beaucoup de coefficients nuls), quand on n'observe que $y=Bw_0+N$ où $B$ est l'opérateur d'observation dans $ \\mathbb{R}^{n \\times p}$ avec $n<<p$ et $N$ est un bruit.  On va supposer ici que $B$ est une matrice aléatoire Gaussienne, ce qui pose notre problème dans le cadre du [compressed sensing](https://en.wikipedia.org/wiki/Compressed_sensing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40\n",
    "p = 60\n",
    "np.random.seed(42)\n",
    "B = np.random.randn(n,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée `w0` le vecteur que l'on souhaite retrouver, qui est parcimonieux et on génère le signal $y=Bw_0+N$ où $N$ est un bruit gaussien additif. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "w0 = np.zeros(p) \n",
    "I=(p*np.random.rand(4)).astype(int)\n",
    "print(I)\n",
    "w0[I] = np.array([.8, -.6, .7, -.9])\n",
    "N = np.random.randn(n)*np.max(np.abs(B@w0))*.02\n",
    "y = (B@w0) + N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fixe $\\lambda = \\frac{\\lambda_{\\max}}{10}$ où $\\lambda_{\\max} = \\Vert B^\\top y \\Vert_\\infty$ est la valeur limite du paramètre pour laquelle on peut montrer que la solution du problème du Lasso est nulle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = np.max(np.abs(B.T @ y))/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On donne aussi un exemple d'utilisation de la fonction `stem` de matplotlib, utile pour représenter les vecteurs parcimonieux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stem( w0, linefmt='--k', markerfmt='ko', label='$x_0$' )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de re-écrire le problème du Lasso comme un problème d'optimisation lisse sans contrainte, on introduit, pour tout vecteur $w\\in \\mathbb{R}^p$, on  introduit le vecteur $x=(x_-,x_+)$ tel que $x_-=\\max(-w,0)$ et $x_+=\\max(w,0)$. On a ainsi toujours \n",
    "$$w = x_+ - x_-\\quad \\text{et}\\quad |w|=x_++x_-\\quad \\text{et}\\quad x\\ge 0.$$\n",
    "Le problème revient donc à minimiser, sous la contrainte $x\\ge 0$ et en décomposant $x=(x_+,x_-)$\n",
    "$$ \n",
    "    f(x) = \\frac{1}{2}\\Vert B(x_+-x_-)-y\\Vert_2^2 + \\lambda \\langle x,1\\rangle,\n",
    "$$ \n",
    "où $1$ est le vecteur rempli de $1$. Ainsi la contrainte s'écrit bien $Ax \\le b$ avec\n",
    "$A=-\\text{Id}_{2p}$, $b=0$.\n",
    "<div class=\"alert alert-block alert-info\"> Calculez ci-dessous le gradient et la Hessienne de $f$ en fonction de $B,y,\\lambda$ et de $x=(x_-,x_+)$  \n",
    "$$\\nabla f(x) = ?? \\quad H[f](x) = ?? $$\n",
    "</div>\n",
    "\n",
    "\n",
    "Remplissez une classe `class lasso()` qui prend au constructeur `B`, `lam` et `y` qui valent respectivement $B$, $\\lambda$ et $y$. Cette classe doit vous calculer $f(x)$, $\\nabla f(x)$ ainsi que la Hessienne dans les fonctions idoines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lasso() :\n",
    "    def __init__(self,B,lam,y) :\n",
    "        pass\n",
    "    def zeros(self) :\n",
    "        pass\n",
    "    def value(self,x) :\n",
    "        pass\n",
    "    def grad(self,x) :\n",
    "        pass\n",
    "    def Hess(self,x) :\n",
    "        pass\n",
    "f=lasso(B,lam,y)\n",
    "np.random.seed(42)\n",
    "x=np.random.randn(2*p)\n",
    "d=np.random.randn(2*p)\n",
    "print('## TEST DE LA FONCTION##')\n",
    "print(f.value(x))                         # 3576.442886034992\n",
    "print(f.grad(x).shape,f.grad(x)[3:6])     # (120,) [ 247.688236   -146.62731787  -37.53567201]\n",
    "print(f.Hess(x).shape,f.Hess(x)[15][3:6]) # (120, 120) [4.15670644 6.06401088 6.55285271]\n",
    "print('## TEST DE DERIVEE NUMERIQUE##')\n",
    "opt.deriv_num(f,x,d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez les classes `lin_cst` et `lasso` définies au dessus pour résoudre le problème du Lasso avec une méthode de Newton, une précision de $10^{-8}$, en partant du point $x_0=1$ pour les différentes valeurs de `t` données dans le tableau `tlist` ci-dessous. Vous afficherez pour chaque `t` le vecteur $w_0$ et sa reconstruction en utilisant le module `plt.stem` donné plus haut dans le notebook. Que remarquez vous sur la solution ? Pouvez-vous l'expliquer ?\n",
    "<div class=\"alert alert-block alert-info\"> VOTRE REPONSE CI-DESSOUS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlist = np.array([1, 10, 100, 1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez ci-dessous la fonction `central_path`. Vous mettrez une tolérance de $10^{-5}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les différentes valeurs de `lam_list` donnez ci-dessous, lancez l'algorithme d'optimisation `central_path` pour résoudre le problème du Lasso. vous mettrez une tolérance de $10^{-4}$. Que remarquez-vous quand $\\lambda$ var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_list = np.array([0.01*lam, 0.1*lam, lam,4*lam])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
